import os
import re
import sys
import time
import json
import pysrt
import ffmpeg
import random
import pickle
import pynvml
import logging
import hashlib
import whisper
import tempfile
import traceback
import configparser
import multiprocessing
from queue import Empty
from pathlib import Path
from datetime import timedelta
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout,
                             QPushButton, QFileDialog, QLineEdit, QGroupBox,
                             QFormLayout, QSpinBox, QTextEdit, QComboBox,
                             QProgressBar, QMessageBox, QCheckBox, QLabel,
                             QDialog, QDialogButtonBox)
from PyQt5.QtCore import QObject, QThread, pyqtSignal, QUrl, Qt
from PyQt5.QtGui import QDesktopServices, QIcon
import dashscope
from dashscope import Generation

try:
    import resources
except ImportError:
    resources = None

class Config:
    APP_NAME = "é¢œè¶£AIé«˜å…‰å‰ªè¾‘å·¥å…·"
    APP_VERSION = "1.6.0"

STYLESHEET = """
QWidget { background-color: #1A1D2A; color: #D0D0D0; font-family: "Segoe UI", "Microsoft YaHei", "PingFang SC", sans-serif; font-size: 10pt; }
QGroupBox { background-color: #24283B; border: 1px solid #25F4EE; border-radius: 8px; margin-top: 1.2em; padding: 15px; }
QGroupBox::title { color: #25F4EE; font-weight: bold; font-size: 11pt; subcontrol-origin: margin; subcontrol-position: top center; padding: 4px 12px; background-color: #1A1D2A; border-radius: 6px; border: 1px solid #25F4EE; }
QPushButton { background-color: #3B3F51; color: #FFFFFF; border: 1px solid #25F4EE; padding: 6px 15px; font-weight: bold; border-radius: 6px; }
QPushButton:hover { background-color: #4A4E60; border-color: #97FEFA; }
QPushButton:pressed { background-color: #25F4EE; color: #1A1D2A; }
QPushButton:disabled { background-color: #4A4E60; color: #888888; border-color: #555555; }
QPushButton#PrimaryButton { background-color: #FE2C55; border-color: #FE2C55; color: #ffffff; font-size: 12pt; font-weight: bold; }
QPushButton#PrimaryButton:hover { background-color: #FF4D71; border-color: #FF4D71; }
QPushButton#PrimaryButton:pressed { background-color: #D92349; }
QTextEdit, QLineEdit { background-color: #1A1D2A; color: #FFFFFF; border: 1px solid #4A4E60; padding: 6px; border-radius: 6px; selection-background-color: #25F4EE; selection-color: #1A1D2A; }
QTextEdit:focus, QLineEdit:focus { border-color: #25F4EE; }
QLabel { color: #E0E0E0; background-color: transparent; }
QLabel#RecommendationLabel { color: #FFD241; font-style: italic; }
QSpinBox, QComboBox { background-color: #3B3F51; border: 1px solid #25F4EE; border-radius: 5px; padding: 5px; color: #FFFFFF; min-width: 6em; }
QComboBox::drop-down { subcontrol-origin: padding; subcontrol-position: top right; width: 20px; border-left-width: 1px; border-left-color: #4A4E60; border-left-style: solid; border-top-right-radius: 5px; border-bottom-right-radius: 5px; }
QComboBox::down-arrow { image: url(:/down_arrow.png); width: 12px; height: 12px; }
QComboBox QAbstractItemView { background-color: #3B3F51; color: #FFFFFF; selection-background-color: #25F4EE; selection-color: #1A1D2A; border: 1px solid #4A4E60; }
QProgressBar { border: 1px solid #4A4E60; border-radius: 5px; text-align: center; color: #FFFFFF; background-color: #3B3F51; height: 10px; }
QProgressBar::chunk { background-color: #25F4EE; border-radius: 5px; }
QScrollBar:vertical, QScrollBar:horizontal { border: none; background-color: #24283B; width: 10px; margin: 0px; }
QScrollBar::handle:vertical, QScrollBar::handle:horizontal { background-color: #4A4E60; border-radius: 5px; min-height: 20px; min-width: 20px; }
QScrollBar::handle:vertical:hover, QScrollBar::handle:horizontal:hover { background-color: #25F4EE; }
QScrollBar::add-line, QScrollBar::sub-line { border: none; background: none; height: 0; width: 0; }
QScrollBar::up-arrow:vertical, QScrollBar::down-arrow:vertical, QScrollBar::left-arrow:horizontal, QScrollBar::right-arrow:horizontal { background: none; }
QScrollBar::add-page:vertical, QScrollBar::sub-page:vertical, QScrollBar::add-page:horizontal, QScrollBar::sub-page:horizontal { background: none; }
QMessageBox { background-color: #24283B; }
QMessageBox QLabel { color: #FFFFFF; background-color: transparent; }
QMenuBar { background-color: #1A1D2A; border-bottom: 1px solid #25F4EE; }
QMenuBar::item { background-color: transparent; color: #D0D0D0; padding: 6px 12px; }
QMenuBar::item:selected, QMenuBar::item:hover { background-color: #3B3F51; }
QMenu { background-color: #24283B; border: 1px solid #4A4E60; padding: 5px; }
QMenu::item { color: #D0D0D0; padding: 8px 25px; }
QMenu::item:selected { background-color: #FE2C55; }
QStatusBar { background-color: #1A1D2A; border-top: 1px solid #25F4EE; }
QStatusBar::item { border: none; }
QCheckBox { spacing: 5px; }
QCheckBox::indicator { width: 18px; height: 18px; border-radius: 4px; }
QCheckBox::indicator:unchecked { background-color: #3B3F51; border: 1px solid #4A4E60; }
QCheckBox::indicator:checked { background-color: #25F4EE; border: 1px solid #25F4EE; image: url(:/check.png); }
"""

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
if not logger.handlers:
    logger.addHandler(logging.StreamHandler())

class Clip(BaseModel):
    id: str
    outline: Dict[str, Any]
    content: List[str]
    start_time: str
    end_time: str
    chunk_index: int
    final_score: float = 0.0
    recommend_reason: str = ""
    generated_title: str = ""
    generated_cover_title: str = ""
    generated_cover_subtitle: str = ""
    keyword_density: float = 0.0

class LLMClient:
    def __init__(self, config):
        self.provider = config.get('api', 'provider')
        self.api_key = config.get('api', 'api_key')
        self.model_name = config.get('api', 'model_name')
        if not self.api_key or self.api_key == 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' or '...' in self.api_key:
            raise ValueError("è¯·åœ¨GUIç•Œé¢æˆ– config.ini æ–‡ä»¶ä¸­é…ç½®æ‚¨çš„æœ‰æ•ˆ API å¯†é’¥ã€‚")
        dashscope.api_key = self.api_key
    def call(self, prompt: str, input_data: Any) -> Optional[str]:
        full_prompt = f"{prompt}\n\n{json.dumps(input_data, ensure_ascii=False, indent=2)}"
        try:
            response = Generation.call(model=self.model_name, prompt=full_prompt, stream=False, use_raw_request=False)
            if response.status_code == 200:
                return response.output.text
            else:
                logger.error(f"API è°ƒç”¨å¤±è´¥: {response.code} - {response.message}")
                return None
        except Exception as e:
            logger.error(f"API è°ƒç”¨æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return None
    def call_with_retry(self, prompt: str, input_data: Any, max_retries: int = 3) -> Optional[str]:
        for attempt in range(max_retries):
            result = self.call(prompt, input_data)
            if result:
                return result
            logger.warning(f"ç¬¬ {attempt + 1} æ¬¡ API è°ƒç”¨è¿”å›ç©ºæˆ–å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")
            time.sleep(2 ** attempt)
        logger.error(f"API è°ƒç”¨åœ¨ {max_retries} æ¬¡é‡è¯•åå½»åº•å¤±è´¥ã€‚")
        return None
    def parse_json_response(self, response: str) -> Optional[Any]:
        cleaned_response = re.sub(r'```json\s*([\s\S]*?)\s*```', r'\1', response.strip())
        try:
            return json.loads(cleaned_response)
        except json.JSONDecodeError:
            logger.error(f"æ— æ³•è§£æ LLM è¿”å›çš„ JSON: {cleaned_response[:200]}...")
            return None

class SubtitleService:
    def __init__(self, log_callback=logging.info):
        self.log_callback = log_callback

    def optimize_line_breaks(self, input_srt_path: str, output_srt_path: str, max_chars_per_line: int = 10, max_lines_per_sub: int = 2) -> bool:
        self.log_callback(f"  ğŸ”„ æ­£åœ¨ä½¿ç”¨æ ‡å‡†ç®—æ³•ä¼˜åŒ–å­—å¹•: {Path(input_srt_path).name}")
        if not Path(input_srt_path).exists():
            self.log_callback(f"  ğŸ”´ é”™è¯¯: å­—å¹•æ–‡ä»¶ '{input_srt_path}' ä¸å­˜åœ¨ã€‚")
            return False
        punctuation_to_remove_at_start = 'ï¼Œã€‚ï¼ï¼Ÿã€,.:;!?'
        try:
            subs = pysrt.open(input_srt_path, encoding='utf-8')
            char_timestamps = []
            full_text_list = []
            for sub in subs:
                text = sub.text_without_tags.strip().replace('\n', ' ')
                if not text: continue
                duration_ms = sub.end.ordinal - sub.start.ordinal
                time_per_char = duration_ms / len(text) if len(text) > 0 else 0
                current_time_ms = sub.start.ordinal
                for char in text:
                    char_timestamps.append(pysrt.SubRipTime.from_ordinal(int(current_time_ms)))
                    current_time_ms += time_per_char
                full_text_list.append(text)
            full_text = " ".join(full_text_list)
            if not full_text:
                self.log_callback("  âš ï¸ è­¦å‘Š: å­—å¹•æ–‡ä»¶å†…å®¹ä¸ºç©ºã€‚")
                with open(output_srt_path, 'w', encoding='utf-8') as f:
                    pass
                return True
            clauses = re.split(r'([ï¼Œã€‚ï¼ï¼Ÿã€,.:;!?])', full_text)
            semantic_clauses = [clauses[i] + (clauses[i+1] if i + 1 < len(clauses) and clauses[i+1] in punctuation_to_remove_at_start else '') for i in range(0, len(clauses), 2) if clauses[i]]
            final_lines = []
            for clause in semantic_clauses:
                if not clause: continue
                while len(clause) > max_chars_per_line:
                    final_lines.append(clause[:max_chars_per_line])
                    clause = clause[max_chars_per_line:]
                if clause:
                    final_lines.append(clause)
            new_subs = pysrt.SubRipFile()
            char_offset = 0
            current_sub_lines = []

            def create_sub_from_block(lines_block, current_char_offset):
                full_line = "".join(lines_block)
                full_line = full_line.strip()
                lines_to_render = full_line.split('\n')
                processed_lines = []
                for l in lines_to_render:
                    l_stripped = l.lstrip(punctuation_to_remove_at_start)
                    l_stripped = l_stripped.lstrip()
                    processed_lines.append(l_stripped)
                new_text = '\n'.join(line for line in processed_lines if line)
                text_length_for_sub = len("".join(lines_block))
                if new_text and current_char_offset + text_length_for_sub <= len(char_timestamps):
                    start_char_idx = current_char_offset
                    end_char_idx = current_char_offset + text_length_for_sub - 1
                    start_time = char_timestamps[start_char_idx]
                    end_time = char_timestamps[end_char_idx]
                    if end_time < start_time:
                        end_time = start_time + timedelta(milliseconds=200 * len(new_text))
                    new_subs.append(pysrt.SubRipItem(index=len(new_subs) + 1, start=start_time, end=end_time, text=new_text))
                return current_char_offset + text_length_for_sub

            for line in final_lines:
                if len(current_sub_lines) < max_lines_per_sub:
                    current_sub_lines.append(line)
                else:
                    char_offset = create_sub_from_block(current_sub_lines, char_offset)
                    current_sub_lines = [line]
            if current_sub_lines:
                create_sub_from_block(current_sub_lines, char_offset)
            new_subs.save(output_srt_path, encoding='utf-8')
            self.log_callback(f"  âœ… å­—å¹•ä¼˜åŒ–å®Œæˆ -> {Path(output_srt_path).name}")
            return True
        except Exception as e:
            self.log_callback(f"  ğŸ”´ å¤„ç†å­—å¹•æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}\n{traceback.format_exc()}")
            return False

class AdvancedSubtitleService:
    def __init__(self, log_callback=logging.info):
        self.log_callback = log_callback

    def optimize_from_word_timestamps(
        self,
        word_segments: List[Dict[str, Any]],
        output_srt_path: str,
        max_chars_per_line: int = 10,
        max_lines_per_sub: int = 2
    ) -> bool:
        self.log_callback(f"  ğŸ”„ æ­£åœ¨ä½¿ç”¨é«˜çº§è¯çº§åˆ«æ—¶é—´æˆ³ç®—æ³•ä¼˜åŒ–å­—å¹•...")
        if not word_segments:
            self.log_callback("  âš ï¸ è­¦å‘Š: è¯çº§åˆ«æ—¶é—´æˆ³æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå­—å¹•ã€‚")
            with open(output_srt_path, 'w', encoding='utf-8') as f:
                pass
            return True
        new_subs = pysrt.SubRipFile()
        current_sub_words = []
        current_lines = []
        current_line_text = ""
        sentence_enders = "ï¼Œã€‚ï¼ï¼Ÿ,.:;!?"
        for word_info in word_segments:
            word_text = word_info.get('word', '').strip()
            if not word_text:
                continue
            potential_line = (current_line_text + word_text).strip()
            line_break = False
            if len(potential_line) > max_chars_per_line:
                line_break = True
            elif current_line_text and any(word_text.startswith(p) for p in sentence_enders):
                line_break = True
            if line_break and current_line_text:
                current_lines.append(current_line_text)
                if len(current_lines) >= max_lines_per_sub:
                    self._create_sub_from_words(new_subs, current_sub_words, current_lines)
                    current_sub_words = []
                    current_lines = []
                current_line_text = word_text
                current_sub_words.append(word_info)
            else:
                current_line_text = potential_line
                current_sub_words.append(word_info)
        if current_line_text:
            current_lines.append(current_line_text)
        if current_sub_words:
            self._create_sub_from_words(new_subs, current_sub_words, current_lines)
        try:
            new_subs.save(output_srt_path, encoding='utf-8')
            self.log_callback(f"  âœ… å­—å¹•ç²¾ç¡®åŒæ­¥ä¼˜åŒ–å®Œæˆ -> {Path(output_srt_path).name}")
            return True
        except Exception as e:
            self.log_callback(f"  ğŸ”´ ä¿å­˜ä¼˜åŒ–å­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def _create_sub_from_words(self, srt_file: pysrt.SubRipFile, words: List[Dict], lines: List[str]):
        if not words or not lines:
            return
        start_time = pysrt.SubRipTime.from_ordinal(int(words[0]['start'] * 1000))
        end_time = pysrt.SubRipTime.from_ordinal(int(words[-1]['end'] * 1000))
        if end_time <= start_time:
            end_time = start_time + timedelta(milliseconds=100 * len("".join(lines)))
        text = "\n".join(line.strip() for line in lines).strip()
        text = re.sub(r'^[ï¼Œã€‚ï¼ï¼Ÿã€,.:;!?]+', '', text)
        if text:
            item = pysrt.SubRipItem(
                index=len(srt_file) + 1,
                start=start_time,
                end=end_time,
                text=text
            )
            srt_file.append(item)

class Processor:
    @staticmethod
    def _write_text_to_temp_file(text: str) -> str:
        try:
            fd, path = tempfile.mkstemp(suffix=".txt", text=False)
            with os.fdopen(fd, 'wb') as f:
                f.write(text.encode('utf-8-sig'))
            return path
        except Exception as e:
            logger.error(f"åˆ›å»ºä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            fd, path = tempfile.mkstemp(suffix=".txt")
            os.close(fd)
            return path

    @staticmethod
    def _escape_path_for_ffmpeg(path_str: str) -> str:
        return str(Path(path_str).resolve()).replace('\\', '/')

    @staticmethod
    def time_to_seconds(time_str: str) -> float:
        try:
            parts = time_str.replace(',', '.').split(':')
            h, m = int(parts[0]), int(parts[1])
            s_ms_parts = parts[2].split('.')
            s, ms = int(s_ms_parts[0]), int(s_ms_parts[1])
            return h * 3600 + m * 60 + s + ms / 1000
        except (IndexError, ValueError) as e:
            logger.error(f"è§£ææ—¶é—´å­—ç¬¦ä¸² '{time_str}'å¤±è´¥: {e}")
            return 0.0

    @staticmethod
    def seconds_to_time_str(seconds: float) -> str:
        h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)
        ms = int((seconds - int(seconds)) * 1000)
        return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

    @staticmethod
    def get_video_duration(video_path: Path) -> float:
        try:
            return float(ffmpeg.probe(str(video_path))['format']['duration'])
        except (ffmpeg.Error, Exception) as e:
            logger.error(f"æ— æ³•è·å–è§†é¢‘ '{video_path.name}' çš„æ—¶é•¿: {getattr(e, 'stderr', e)}")
            return 0.0

    @staticmethod
    def parse_srt(srt_path: Path) -> List[Dict]:
        with open(srt_path, 'r', encoding='utf-8') as f:
            content = f.read()
        entries = []
        for block in content.strip().split('\n\n'):
            lines = block.split('\n')
            if len(lines) >= 3:
                try:
                    time_match = re.match(r'(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})', lines[1])
                    if time_match:
                        start, end = time_match.groups()
                        text = ' '.join(lines[2:])
                        entries.append({"index": int(lines[0]), "start_time": start, "end_time": end, "text": text})
                except (ValueError, IndexError):
                    continue
        return entries

    @staticmethod
    def cut_video_clip(
        source_path: Path,
        output_path: Path,
        clip_data: Clip,
        clip_start_in_file: float,
        clip_end_in_file: float,
        convert_landscape_to_portrait: bool,
        subtitle_path: Optional[Path] = None,
        font_file: Optional[str] = None,
        add_bgm: bool = False,
        vcodec: str = 'libx264',
        preset: str = 'medium',
        crf: int = 18
    ):
        main_title_file = None
        sub_title_file = None
        try:
            if clip_start_in_file < 0 or clip_end_in_file <= clip_start_in_file:
                logger.error(f"è·³è¿‡å‰ªè¾‘ï¼Œæ— æ•ˆçš„æ—¶é—´æˆ³: start={clip_start_in_file:.2f}, end={clip_end_in_file:.2f} for {output_path.name}")
                return
            duration = clip_end_in_file - clip_start_in_file
            if duration <= 0:
                logger.error(f"è·³è¿‡å‰ªè¾‘ï¼Œè®¡ç®—å‡ºçš„æ—¶é•¿ä¸ºé›¶æˆ–è´Ÿæ•°: {duration:.2f}s for {output_path.name}")
                return
            probe_info = ffmpeg.probe(str(source_path))
            video_info = next((s for s in probe_info['streams'] if s['codec_type'] == 'video'), None)
            if not video_info:
                logger.error(f"æ— æ³•æ‰¾åˆ°è§†é¢‘æµ: {source_path.name}")
                return
            src_width, src_height = video_info['width'], video_info['height']
            is_landscape = src_width > src_height
            ffmpeg_executable = "ffmpeg.exe" if sys.platform == "win32" else "ffmpeg"
            stream = ffmpeg.input(str(source_path), ss=clip_start_in_file, t=duration)
            video_stream = stream.video
            audio_stream = stream.audio
            can_use_filters = vcodec != 'copy'
            color_palette = ["#FF6A41", "#FF7841", "#FF8C41", "#FF9A41", "#FFA841", "#FFB641", "#FFC441", "#FFD241", "#FFE041", "#FFEC41"]
            font_dir_path = Path.cwd() / "fonts"
            if not is_landscape or convert_landscape_to_portrait:
                logger.info(f"ä¸º {output_path.name} åº”ç”¨ç«–å±(9:16)å¤„ç†æµç¨‹ã€‚")
                width, height = 1080, 1920
                if can_use_filters:
                    video_stream = (
                        video_stream
                        .filter('scale', width, height, force_original_aspect_ratio='decrease')
                        .filter('pad', width, height, x='(ow-iw)/2', y='(oh-ih)/2', color='black')
                        .filter('setsar', 1)
                    )
                if can_use_filters and font_dir_path.is_dir() and font_file and clip_data.generated_cover_title:
                    main_title_size, sub_title_size = 96, 72
                    font_path_escaped = Processor._escape_path_for_ffmpeg(str(font_dir_path / font_file))
                    title_color = random.choice(color_palette)
                    main_title_file = Processor._write_text_to_temp_file(clip_data.generated_cover_title)
                    video_stream = video_stream.drawtext(
                        textfile=Processor._escape_path_for_ffmpeg(main_title_file),
                        reload=1, fontfile=font_path_escaped,
                        fontsize=main_title_size, fontcolor=title_color,
                        x='(w-text_w)/2', y='h*0.22'
                    )
                    if clip_data.generated_cover_subtitle:
                        sub_title_file = Processor._write_text_to_temp_file(clip_data.generated_cover_subtitle)
                        video_stream = video_stream.drawtext(
                            textfile=Processor._escape_path_for_ffmpeg(sub_title_file),
                            reload=1, fontfile=font_path_escaped,
                            fontsize=sub_title_size, fontcolor=title_color,
                            x='(w-text_w)/2', y=f'h*0.22 + {main_title_size}*1.2'
                        )
                if can_use_filters and subtitle_path and subtitle_path.exists() and subtitle_path.stat().st_size > 0 and font_file:
                    sub_color_hex = random.choice(color_palette)
                    ass_color_bgr = f"{sub_color_hex[5:7]}{sub_color_hex[3:5]}{sub_color_hex[1:3]}"
                    style = f"FontName={Path(font_file).stem},PrimaryColour=&H00{ass_color_bgr},BorderStyle=1,Outline=2,Alignment=2,MarginV=65"
                    video_stream = ffmpeg.filter(
                        video_stream, 'subtitles',
                        filename=Processor._escape_path_for_ffmpeg(str(subtitle_path)),
                        force_style=style,
                        fontsdir=Processor._escape_path_for_ffmpeg(str(font_dir_path))
                    )
            else:
                logger.info(f"ä¸º {output_path.name} åº”ç”¨ä¿æŒæ¨ªå±å®½é«˜æ¯”(æ ‡é¢˜å­—å¹•çƒ§å½•åœ¨ç”»é¢ä¸Š)çš„å¤„ç†æµç¨‹ã€‚")
                if can_use_filters and font_dir_path.is_dir() and font_file and clip_data.generated_cover_title:
                    main_title_size = int(src_height * 0.06)
                    sub_title_size = int(src_height * 0.04)
                    font_path_escaped = Processor._escape_path_for_ffmpeg(str(font_dir_path / font_file))
                    title_color = random.choice(color_palette)
                    border_width = max(2, int(main_title_size / 20))
                    main_title_y_pos = 'h*0.1' # è·ç¦»é¡¶éƒ¨10%çš„é«˜åº¦
                    main_title_file = Processor._write_text_to_temp_file(clip_data.generated_cover_title)
                    video_stream = video_stream.drawtext(
                        textfile=Processor._escape_path_for_ffmpeg(main_title_file),
                        reload=1, fontfile=font_path_escaped,
                        fontsize=main_title_size, fontcolor=title_color,
                        x='(w-text_w)/2', y=main_title_y_pos,
                        borderw=border_width, bordercolor='black', # æ·»åŠ æè¾¹
                        shadowx=2, shadowy=2, shadowcolor='black@0.6' # æ·»åŠ é˜´å½±
                    )
                    if clip_data.generated_cover_subtitle:
                        sub_border_width = max(2, int(sub_title_size / 20))
                        sub_title_y_pos = f'{main_title_y_pos} + {main_title_size}*1.2'
                        sub_title_file = Processor._write_text_to_temp_file(clip_data.generated_cover_subtitle)
                        video_stream = video_stream.drawtext(
                            textfile=Processor._escape_path_for_ffmpeg(sub_title_file),
                            reload=1, fontfile=font_path_escaped,
                            fontsize=sub_title_size, fontcolor=title_color,
                            x='(w-text_w)/2', y=sub_title_y_pos,
                            borderw=sub_border_width, bordercolor='black',
                            shadowx=2, shadowy=2, shadowcolor='black@0.6'
                        )
                if can_use_filters and subtitle_path and subtitle_path.exists() and subtitle_path.stat().st_size > 0 and font_file:
                    sub_color_hex = random.choice(color_palette)
                    ass_color_bgr = f"{sub_color_hex[5:7]}{sub_color_hex[3:5]}{sub_color_hex[1:3]}"
                    subtitle_font_size = int(src_height * 0.02)
                    subtitle_margin_v = int(src_height * 0.02)
                    style = (f"FontName={Path(font_file).stem},"
                             f"FontSize={subtitle_font_size},"
                             f"PrimaryColour=&H00{ass_color_bgr},"
                             f"BorderStyle=1,Outline=2,Shadow=1," # åŠ ç²—è¾¹æ¡†å’Œé˜´å½±
                             f"Alignment=2," # 2=åº•éƒ¨å±…ä¸­
                             f"MarginV={subtitle_margin_v}") # è·ç¦»åº•éƒ¨çš„è¾¹è·
                    video_stream = ffmpeg.filter(
                        video_stream, 'subtitles',
                        filename=Processor._escape_path_for_ffmpeg(str(subtitle_path)),
                        force_style=style,
                        fontsdir=Processor._escape_path_for_ffmpeg(str(font_dir_path))
                    )
            if add_bgm:
                bgms_dir = Path.cwd() / "bgms"
                if bgms_dir.is_dir():
                    bgm_files = list(bgms_dir.glob("*.mp3")) + list(bgms_dir.glob("*.wav"))
                    if bgm_files:
                        bgm_path = random.choice(bgm_files)
                        bgm_stream = (
                            ffmpeg.input(str(bgm_path), stream_loop=-1)
                            .audio.filter('atrim', duration=duration)
                            .filter('volume', 0.2)
                        )
                        audio_stream = ffmpeg.filter([audio_stream, bgm_stream], 'amix', inputs=2)
                        logger.info(f"å·²ä¸ºç‰‡æ®µæ·»åŠ BGM: {bgm_path.name}")
                    else:
                        logger.warning(f"å‹¾é€‰äº†æ·»åŠ BGMï¼Œä½† '{bgms_dir}' æ–‡ä»¶å¤¹ä¸ºç©ºã€‚")
            output_options = {
                'vcodec': vcodec, 'acodec': 'aac', 'audio_bitrate': '192k', 'strict': 'experimental'
            }
            if vcodec != 'copy':
                output_options['preset'] = preset
                output_options['crf'] = crf
            else:
                 logger.warning(f"ä½¿ç”¨ 'copy' ç¼–ç å™¨ï¼Œå°†è·³è¿‡æ‰€æœ‰è§†é¢‘æ»¤é•œï¼ˆç¼©æ”¾ã€å¡«å……ã€æ ‡é¢˜ã€å­—å¹•ï¼‰ã€‚")
            (
                ffmpeg.output(video_stream, audio_stream, str(output_path), **output_options)
                .run(cmd=ffmpeg_executable, overwrite_output=True, quiet=True)
            )
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆç‰‡æ®µ: {output_path.name} (æ—¶é•¿: {duration:.2f}s)")
        except ffmpeg.Error as e:
            logger.error(f"FFmpeg å‰ªè¾‘å¤±è´¥: {output_path.name}\n{e.stderr.decode('utf-8', 'replace') if e.stderr else 'N/A'}")
        except Exception as e:
            logger.error(f"å‰ªè¾‘è§†é¢‘æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\n{traceback.format_exc()}")
        finally:
            if main_title_file and os.path.exists(main_title_file):
                os.remove(main_title_file)
            if sub_title_file and os.path.exists(sub_title_file):
                os.remove(sub_title_file)

    @staticmethod
    def generate_sliding_window_clips(srt_data: List[Dict], target_duration: int, tolerance: float, slide_step: int) -> List[Dict]:
        clips = []
        min_duration = target_duration * (1 - tolerance)
        max_duration = target_duration * (1 + tolerance)
        if not srt_data:
            return []
        next_start_time = 0.0
        for i in range(len(srt_data)):
            start_entry_time = Processor.time_to_seconds(srt_data[i]['start_time'])
            if start_entry_time < next_start_time:
                continue
            current_window_start_time = start_entry_time
            current_content = []
            for j in range(i, len(srt_data)):
                if (Processor.time_to_seconds(srt_data[j]['end_time']) - current_window_start_time) < max_duration:
                    current_content.append(srt_data[j])
                else:
                    current_content.append(srt_data[j])
                    break
            if not current_content:
                continue
            if (Processor.time_to_seconds(current_content[-1]['end_time']) - Processor.time_to_seconds(current_content[0]['start_time'])) >= min_duration:
                clips.append({
                    "outline": {"title": " ".join([e['text'] for e in current_content])[:30] + "...", "subtopics": []},
                    "content": [e['text'] for e in current_content],
                    "start_time": current_content[0]['start_time'],
                    "end_time": current_content[-1]['end_time']
                })
                next_start_time = current_window_start_time + slide_step
        return clips

class AIPipeline:
    def __init__(self, config: configparser.ConfigParser, temp_dir: Path, clip_params: Dict, progress_signal = None):
        self.clip_params = clip_params
        self.llm = LLMClient(config)
        self.processor = Processor()
        self.temp_dir = temp_dir
        self.max_concurrent_tasks = config.getint('settings', 'max_concurrent_tasks', fallback=10)
        self.prompts = {}
        prompt_dir = Path(__file__).parent / "prompts"
        for key in ["æ¨èç†ç”±", "æ ‡é¢˜ç”Ÿæˆ"]:
            prompt_file = prompt_dir / f"{key}.txt"
            if not prompt_file.exists():
                raise FileNotFoundError(f"æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°: {prompt_file}")
            with open(prompt_file, 'r', encoding='utf-8') as f:
                self.prompts[key] = f.read()
        self.progress_callback = progress_signal
    def _emit_progress(self, start_percent: int, current_step: int, total_steps: int, step_weight: int):
        if self.progress_callback and total_steps > 0:
            self.progress_callback(start_percent + int((current_step / total_steps) * step_weight))
    def _batch_list(self, data: List[Any], batch_size: int) -> List[List[Any]]:
        return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]
    def run(self, srt_path: Path) -> List[Clip]:
        srt_data = self.processor.parse_srt(srt_path)
        target_duration, tolerance = self.clip_params['clip_duration'], self.clip_params['duration_tolerance']
        slide_step = max(1, int(target_duration / 2))
        logger.info(f"--- æ­£åœ¨ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆå€™é€‰ç‰‡æ®µ (æ—¶é•¿: {target_duration}s, æ­¥é•¿: {slide_step}s, å®¹å¿åº¦: {tolerance:.2f}) ---")
        candidate_clips_data = self.processor.generate_sliding_window_clips(srt_data, target_duration, tolerance, slide_step)
        if not candidate_clips_data:
            logger.warning("æœªèƒ½ç”Ÿæˆä»»ä½•ç¬¦åˆæ—¶é•¿è¦æ±‚çš„å€™é€‰ç‰‡æ®µã€‚")
            return []
        logger.info(f"ç”Ÿæˆäº† {len(candidate_clips_data)} ä¸ªå€™é€‰ç‰‡æ®µï¼Œå‡†å¤‡è¿›è¡ŒAIè¯„åˆ†ã€‚")
        scored_outlines = self._score_clips(candidate_clips_data) or candidate_clips_data
        logger.info("å®Œæˆå†…å®¹è¯„åˆ†ã€‚")
        all_clips = [Clip(id=str(i), chunk_index=0, **item) for i, item in enumerate(scored_outlines)]
        for clip in all_clips:
            if 'final_score' not in clip.model_dump():
                clip.final_score = 0.0
            if 'recommend_reason' not in clip.model_dump():
                clip.recommend_reason = "N/A"
        if not all_clips:
            return []
        logger.info("--- æ­£åœ¨ä¸ºæ‰€æœ‰å€™é€‰ç‰‡æ®µç”Ÿæˆæ ‡é¢˜ ---")
        clips_with_titles = self._generate_titles(all_clips)
        logger.info("æ ‡é¢˜ç”Ÿæˆå®Œæ¯•ã€‚")
        return clips_with_titles
    def _score_clips(self, clips: List[Dict]) -> List[Dict]:
        clip_batches = self._batch_list(clips, 20)
        all_scored_clips = []
        total_batches = len(clip_batches)
        logger.info(f"ç‰‡æ®µå°†è¢«åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶å‘è¯„åˆ† (æœ€å¤š {self.max_concurrent_tasks} ä¸ªå¹¶å‘ä»»åŠ¡)ã€‚")
        start_progress, weight, completed_batches = 30, 35, 0
        with ThreadPoolExecutor(max_workers=self.max_concurrent_tasks) as executor:
            future_to_index = {executor.submit(self.llm.call_with_retry, self.prompts['æ¨èç†ç”±'], batch): i for i, batch in enumerate(clip_batches)}
            for future in as_completed(future_to_index):
                batch_index = future_to_index[future]
                completed_batches += 1
                try:
                    response = future.result()
                    if response and (parsed_json := self.llm.parse_json_response(response)) and isinstance(parsed_json, list):
                        all_scored_clips.extend(parsed_json)
                        logger.info(f"æ‰¹æ¬¡ {batch_index + 1}/{total_batches} è¯„åˆ†æˆåŠŸã€‚")
                    else:
                        logger.warning(f"æ— æ³•è§£ææˆ–APIè°ƒç”¨å¤±è´¥ï¼Œæ‰¹æ¬¡ {batch_index + 1} çš„è¯„åˆ†å“åº”ã€‚")
                except Exception as exc:
                    logger.error(f"å¤„ç†è¯„åˆ†æ‰¹æ¬¡ {batch_index + 1} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                finally:
                    self._emit_progress(start_progress, completed_batches, total_batches, weight)
        self._emit_progress(start_progress, total_batches, total_batches, weight)
        return all_scored_clips
    def _generate_titles(self, clips: List[Clip]) -> List[Clip]:
        if not clips:
            return []
        clip_batches = self._batch_list(clips, 10)
        clips_by_id = {c.id: c for c in clips}
        total_batches = len(clip_batches)
        logger.info(f"ç‰‡æ®µå°†è¢«åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¹¶å‘ç”Ÿæˆæ ‡é¢˜ã€‚")
        start_progress, weight, completed_batches = 65, 15, 0
        with ThreadPoolExecutor(max_workers=self.max_concurrent_tasks) as executor:
            batch_inputs = [
                {"clips": [{"id": c.id, "content": " ".join(c.content), "recommend_reason": c.recommend_reason} for c in batch]}
                for batch in clip_batches
            ]
            future_to_index = {
                executor.submit(self.llm.call_with_retry, self.prompts['æ ‡é¢˜ç”Ÿæˆ'], batch_input): i
                for i, batch_input in enumerate(batch_inputs)
            }
            for future in as_completed(future_to_index):
                batch_index = future_to_index[future]
                completed_batches += 1
                try:
                    response = future.result()
                    if response and (titles_data := self.llm.parse_json_response(response)) and isinstance(titles_data, dict):
                        for clip_id, title_info in titles_data.items():
                            if clip_id in clips_by_id and isinstance(title_info, dict):
                                clips_by_id[clip_id].generated_title = title_info.get("title", "æœªç”Ÿæˆæ ‡é¢˜")
                                clips_by_id[clip_id].generated_cover_title = title_info.get("cover_title", "")
                                clips_by_id[clip_id].generated_cover_subtitle = title_info.get("cover_subtitle", "")
                        logger.info(f"æ‰¹æ¬¡ {batch_index + 1}/{total_batches} æ ‡é¢˜ç”ŸæˆæˆåŠŸã€‚")
                    else:
                        logger.warning(f"æ— æ³•è§£ææˆ–APIè°ƒç”¨å¤±è´¥ï¼Œæ‰¹æ¬¡ {batch_index + 1} çš„æ ‡é¢˜å“åº”ã€‚")
                except Exception as exc:
                    logger.error(f"å¤„ç†æ ‡é¢˜ç”Ÿæˆæ‰¹æ¬¡ {batch_index + 1} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                finally:
                    self._emit_progress(start_progress, completed_batches, total_batches, weight)
        self._emit_progress(start_progress, total_batches, total_batches, weight)
        return list(clips_by_id.values())

def run_processing_task(queue, params):
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.handlers.clear()
    queue_handler = QueueHandler(queue)
    queue_handler.setFormatter(formatter)
    root_logger.addHandler(queue_handler)
    global logger
    logger = logging.getLogger('worker_process')
    def progress_callback(value):
        queue.put(('progress', value))
    try:
        config = configparser.ConfigParser()
        config.read(params['config_path'], encoding='utf-8')
        input_path = Path(params['input_path'])
        output_dir = Path(params['output_dir'])
        output_dir.mkdir(exist_ok=True, parents=True)
        temp_dir = output_dir / "temp"
        temp_dir.mkdir(exist_ok=True)
        cache_dir = Path.cwd() / "cache"
        cache_dir.mkdir(exist_ok=True)
        progress_callback(5)
        video_files = _get_video_files(input_path)
        if not video_files:
            raise FileNotFoundError("åœ¨æŒ‡å®šè·¯å¾„ä¸‹æœªæ‰¾åˆ°æœ‰æ•ˆçš„è§†é¢‘æ–‡ä»¶ã€‚")
        use_precise_subtitles = params.get('precise_subtitles', False)
        cache_key_str = "".join([f"{p.resolve()}{p.stat().st_mtime}" for p in video_files]) + str(use_precise_subtitles)
        cache_key = hashlib.md5(cache_key_str.encode()).hexdigest()
        cache_file = cache_dir / f"{cache_key}.json"
        all_potential_clips = []
        if cache_file.exists():
            logger.info(f"ğŸ‰ å‘ç°AIåˆ†æç»“æœç¼“å­˜: {cache_file.name}ï¼Œæ­£åœ¨åŠ è½½...")
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                all_potential_clips = [Clip.model_validate(item) for item in cached_data]
            logger.info("âœ… ç¼“å­˜åŠ è½½æˆåŠŸï¼Œè·³è¿‡AIåˆ†ææ­¥éª¤ã€‚")
            progress_callback(80)
        else:
            logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„AIåˆ†æç¼“å­˜ï¼Œå°†æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹ã€‚")
            combined_srt_name = f"{cache_key}_combined.srt"
            srt_path = temp_dir / combined_srt_name
            _generate_and_combine_srts(video_files, srt_path, temp_dir, params, progress_callback)
            progress_callback(30)
            pipeline = AIPipeline(config, temp_dir, params, progress_signal=progress_callback)
            all_potential_clips = pipeline.run(srt_path)
            if all_potential_clips:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump([clip.model_dump() for clip in all_potential_clips], f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… AIåˆ†æç»“æœå·²ç¼“å­˜è‡³: {cache_file.name}")
        if not all_potential_clips:
            raise RuntimeError("AIåˆ†ææœªèƒ½ç”Ÿæˆä»»ä½•å€™é€‰ç‰‡æ®µã€‚")
        logger.info(f"AIåˆ†æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_potential_clips)} ä¸ªå€™é€‰ç‰‡æ®µã€‚")
        num_clips = params['num_clips']
        logger.info(f"å¼€å§‹ç­›é€‰ç‰‡æ®µï¼šæ•°é‡={num_clips}")
        power_words = params.get('power_words', [])
        def is_overlapping(clip1, clip2, threshold=0.8):
            start1, end1 = Processor.time_to_seconds(clip1.start_time), Processor.time_to_seconds(clip1.end_time)
            start2, end2 = Processor.time_to_seconds(clip2.start_time), Processor.time_to_seconds(clip2.end_time)
            overlap_start = max(start1, start2)
            overlap_end = min(end1, end2)
            if overlap_start < overlap_end:
                overlap_duration = overlap_end - overlap_start
                min_duration = min(end1 - start1, end2 - start2)
                if min_duration > 0 and (overlap_duration / min_duration) > threshold:
                    return True
            return False
        use_keyword_sorting = bool(power_words)
        if use_keyword_sorting:
            logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯è¿›è¡Œè¾…åŠ©æ’åº: {', '.join(power_words)}")
            for clip in all_potential_clips:
                content_text = "".join(clip.content)
                keyword_count = sum(content_text.count(word) for word in power_words)
                clip.keyword_density = keyword_count / len(content_text) if len(content_text) > 0 else 0
        else:
            logger.info("æœªæä¾›é«˜å…‰å…³é”®è¯ï¼Œä»…æŒ‰åˆ†æ•°æ’åºã€‚")
            for clip in all_potential_clips:
                clip.keyword_density = 0
        all_potential_clips.sort(key=lambda c: (c.final_score, c.keyword_density), reverse=True)
        final_clips = []
        for clip in all_potential_clips:
            if len(final_clips) >= num_clips: break
            if not any(is_overlapping(clip, selected_clip) for selected_clip in final_clips):
                final_clips.append(clip)
        logger.info(f"å·²é€šè¿‡æ™ºèƒ½ç­›é€‰å’Œå»é‡ï¼Œé€‰å‡º {len(final_clips)} ä¸ªæœ€ä½³ç‰‡æ®µå‡†å¤‡å‰ªè¾‘ã€‚")
        burn_subtitles = params.get('burn_subtitles', False)
        font_file_name = None
        if burn_subtitles:
            font_dir = Path.cwd() / "fonts"
            if font_dir.is_dir():
                font_files = list(font_dir.glob('*.ttf')) + list(font_dir.glob('*.otf'))
                if font_files:
                    font_file_name = font_files[0].name
                    logger.info(f"æ‰¾åˆ°å¹¶è®¾ç½®å­—ä½“: {font_file_name}")
                else:
                    logger.warning("å‹¾é€‰äº†æ·»åŠ æ ‡é¢˜å’Œå­—å¹•,ä½† 'fonts' æ–‡ä»¶å¤¹ä¸ºç©ºã€‚å°†ä¸æ·»åŠ ã€‚")
                    burn_subtitles = False
            else:
                logger.warning("å‹¾é€‰äº†æ·»åŠ æ ‡é¢˜å’Œå­—å¹•,ä½† 'fonts' æ–‡ä»¶å¤¹ä¸å­˜åœ¨ã€‚å°†ä¸æ·»åŠ ã€‚")
                burn_subtitles = False
        progress_callback(80)
        all_word_segments = []
        if burn_subtitles and use_precise_subtitles:
            word_segments_path = temp_dir / f"{cache_key}_word_segments.pkl"
            if word_segments_path.exists():
                logger.info("æ­£åœ¨åŠ è½½ç²¾ç¡®çš„è¯çº§åˆ«æ—¶é—´æˆ³æ•°æ®ç”¨äºå­—å¹•ç”Ÿæˆ...")
                with open(word_segments_path, 'rb') as f:
                    all_word_segments = pickle.load(f)
            else:
                logger.warning("æœªæ‰¾åˆ°è¯çº§åˆ«æ—¶é—´æˆ³æ•°æ®ï¼Œå°†å›é€€åˆ°æ ‡å‡†å­—å¹•æ¨¡å¼ã€‚")
                use_precise_subtitles = False
        srt_combined_path = temp_dir / f"{cache_key}_combined.srt"
        srt_data = []
        if burn_subtitles and not use_precise_subtitles:
            if srt_combined_path.exists():
                srt_data = Processor.parse_srt(srt_combined_path)
            else:
                logger.warning("æœªæ‰¾åˆ°åˆå¹¶çš„SRTæ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆæ ‡å‡†å­—å¹•ã€‚")
        add_bgm_enabled = params.get('add_bgm', False)
        vcodec = params.get('vcodec', 'h264_nvenc' if params.get('use_gpu') else 'libx264')
        default_preset = 'p4' if vcodec in ['h264_nvenc', 'hevc_nvenc'] else 'medium'
        preset = params.get('preset', default_preset)
        crf = params.get('crf', 18)
        max_subtitle_lines = params.get('max_subtitle_lines', 2)
        max_chars_per_line = params.get('max_chars_per_line', 10)
        convert_landscape = params.get('convert_landscape_to_portrait', True)
        for i, clip in enumerate(final_clips):
            coarse_start_s = Processor.time_to_seconds(clip.start_time)
            coarse_end_s = Processor.time_to_seconds(clip.end_time)
            clip_subtitle_path = None
            precise_start_s, precise_end_s = coarse_start_s, coarse_end_s
            if burn_subtitles and font_file_name:
                if use_precise_subtitles and all_word_segments:
                    clip_word_segments = [word for word in all_word_segments if max(word['start'], coarse_start_s) < min(word['end'], coarse_end_s)]
                    if clip_word_segments:
                        precise_start_s = clip_word_segments[0]['start']
                        precise_end_s = clip_word_segments[-1]['end']
                        logger.info(f"ç‰‡æ®µ {i+1} æ—¶é—´å·²ç²¾ç‚¼: [{coarse_start_s:.2f}, {coarse_end_s:.2f}] -> [{precise_start_s:.2f}, {precise_end_s:.2f}]")
                        adjusted_clip_words = []
                        for word in clip_word_segments:
                            adj_word = word.copy()
                            adj_word['start'] -= precise_start_s
                            adj_word['end'] -= precise_start_s
                            adjusted_clip_words.append(adj_word)
                        subtitle_optimizer = AdvancedSubtitleService(log_callback=logger.info)
                        optimized_clip_srt_path = temp_dir / f"optimized_clip_{i+1:02d}.srt"
                        success = subtitle_optimizer.optimize_from_word_timestamps(
                            word_segments=adjusted_clip_words, output_srt_path=str(optimized_clip_srt_path),
                            max_chars_per_line=max_chars_per_line, max_lines_per_sub=max_subtitle_lines
                        )
                        if success and optimized_clip_srt_path.exists() and optimized_clip_srt_path.stat().st_size > 0:
                            clip_subtitle_path = optimized_clip_srt_path
                        else:
                            logger.warning(f"ä¸ºç‰‡æ®µ {i+1} ç”Ÿæˆç²¾ç¡®åŒæ­¥å­—å¹•å¤±è´¥ã€‚")
                elif srt_data:
                    clip_srt_content, sub_index = "", 1
                    for entry in srt_data:
                        entry_start_s, entry_end_s = Processor.time_to_seconds(entry['start_time']), Processor.time_to_seconds(entry['end_time'])
                        if max(coarse_start_s, entry_start_s) < min(coarse_end_s, entry_end_s):
                            new_start_s = max(0, entry_start_s - coarse_start_s)
                            new_end_s = max(0, entry_end_s - coarse_start_s)
                            if new_end_s > new_start_s:
                                clip_srt_content += f"{sub_index}\n{Processor.seconds_to_time_str(new_start_s)} --> {Processor.seconds_to_time_str(new_end_s)}\n{entry['text']}\n\n"
                                sub_index += 1
                    if clip_srt_content:
                        raw_clip_srt_path = temp_dir / f"raw_clip_{i+1:02d}.srt"
                        with open(raw_clip_srt_path, 'w', encoding='utf-8') as f: f.write(clip_srt_content)
                        if raw_clip_srt_path.stat().st_size > 0:
                            subtitle_optimizer = SubtitleService(log_callback=logger.info)
                            optimized_clip_srt_path = temp_dir / f"optimized_clip_{i+1:02d}.srt"
                            success = subtitle_optimizer.optimize_line_breaks(
                                str(raw_clip_srt_path), str(optimized_clip_srt_path),
                                max_chars_per_line=max_chars_per_line, max_lines_per_sub=max_subtitle_lines
                            )
                            clip_subtitle_path = optimized_clip_srt_path if success else raw_clip_srt_path
            current_offset, source_file_found = 0.0, False
            for video_file in video_files:
                video_duration = Processor.get_video_duration(video_file)
                if precise_start_s < current_offset + video_duration:
                    clip_start_in_file = precise_start_s - current_offset
                    clip_end_in_file = precise_end_s - current_offset
                    safe_title = re.sub(r'[\\/*?:"<>|]', "", clip.generated_title)
                    output_filename = f"{i+1:02d}_{safe_title[:50]}.mp4"
                    Processor.cut_video_clip(
                        source_path=video_file, output_path=output_dir / output_filename,
                        clip_data=clip, clip_start_in_file=clip_start_in_file,
                        clip_end_in_file=clip_end_in_file,
                        convert_landscape_to_portrait=convert_landscape,
                        subtitle_path=clip_subtitle_path,
                        font_file=font_file_name, add_bgm=add_bgm_enabled,
                        vcodec=vcodec, preset=preset, crf=crf
                    )
                    source_file_found = True
                    break
                else:
                    current_offset += video_duration
            if not source_file_found:
                logger.warning(f"æœªèƒ½ä¸ºç‰‡æ®µ '{clip.generated_title}' æ‰¾åˆ°æºè§†é¢‘æ–‡ä»¶ã€‚")
            progress_callback(80 + int(20 * (i + 1) / len(final_clips)))
        logger.info(f"ğŸ‰ æ‰€æœ‰é«˜å…‰ç‰‡æ®µå·²ç”Ÿæˆï¼æ–‡ä»¶ä¿å­˜åœ¨ç›®å½•: {output_dir.resolve()}")
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        queue.put(('error', str(e)))
    finally:
        queue.put(('finished', None))

class QueueHandler(logging.Handler):
    def __init__(self, queue):
        super().__init__()
        self.queue = queue
    def emit(self, record):
        self.queue.put(('log', self.format(record)))

class PyQtLogHandler(logging.Handler):
    def __init__(self, log_signal):
        super().__init__()
        self.log_signal = log_signal
    def emit(self, record):
        self.log_signal.emit(self.format(record))

def _get_video_files(input_path: Path) -> List[Path]:
    VIDEO_EXTENSIONS = ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv']
    if input_path.is_dir():
        files = sorted([p for p in input_path.iterdir() if p.suffix.lower() in VIDEO_EXTENSIONS])
        return files
    elif input_path.is_file() and input_path.suffix.lower() in VIDEO_EXTENSIONS:
        return [input_path]
    return []

def _generate_and_combine_srts(video_files: List[Path], combined_srt_path: Path, temp_dir: Path, params: dict, progress_callback):
    language_to_use = params.get('language')
    whisper_model_name = params.get('whisper_model', 'medium')
    use_precise_subtitles = params.get('precise_subtitles', False)
    logger.info(f"æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹: {whisper_model_name}...")
    whisper_model = whisper.load_model(whisper_model_name)
    logger.info("Whisper æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")
    total_offset_seconds, combined_srt_content, entry_index = 0.0, "", 1
    all_word_segments = []
    transcribe_progress_total_weight = 25
    for i, video_file in enumerate(video_files):
        logger.info(f"--- å¼€å§‹å¤„ç†è§†é¢‘ {i+1}/{len(video_files)}: {video_file.name} ---")
        result = None
        individual_word_segments_path = temp_dir / f"{video_file.stem}_word_segments.pkl"
        individual_srt_path = temp_dir / f"{video_file.stem}.srt"
        if use_precise_subtitles and individual_word_segments_path.exists():
            logger.info(f"å‘ç°å·²å­˜åœ¨çš„è¯çº§åˆ«æ—¶é—´æˆ³ç¼“å­˜: '{individual_word_segments_path.name}'ï¼Œå°†ç›´æ¥åŠ è½½ã€‚")
            with open(individual_word_segments_path, 'rb') as f: result = pickle.load(f)
        elif not use_precise_subtitles and individual_srt_path.exists():
            logger.info(f"å‘ç°å·²å­˜åœ¨çš„SRTæ–‡ä»¶: '{individual_srt_path.name}'ï¼Œå°†ç›´æ¥ä½¿ç”¨ã€‚")
            result = {'segments': [{'start': Processor.time_to_seconds(e['start_time']), 'end': Processor.time_to_seconds(e['end_time']), 'text': e['text']} for e in Processor.parse_srt(individual_srt_path)]}
        if not result:
            log_msg = f"æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ« (è¯­è¨€: {language_to_use or 'è‡ªåŠ¨æ£€æµ‹'})"
            if use_precise_subtitles: log_msg += ", å¼€å¯è¯çº§åˆ«æ—¶é—´æˆ³"
            logger.info(log_msg + "...")
            transcribe_args = {'language': language_to_use} if language_to_use else {}
            if use_precise_subtitles: transcribe_args['word_timestamps'] = True
            result = whisper_model.transcribe(str(video_file), verbose=False, **transcribe_args)
            if use_precise_subtitles and 'segments' in result:
                with open(individual_word_segments_path, 'wb') as f: pickle.dump(result, f)
                logger.info(f"å·²ä¸º '{video_file.name}' ç”Ÿæˆå¹¶ç¼“å­˜è¯çº§åˆ«æ—¶é—´æˆ³æ•°æ®ã€‚")
            individual_srt_content = "".join(f"{seg_idx+1}\n{Processor.seconds_to_time_str(segment['start'])} --> {Processor.seconds_to_time_str(segment['end'])}\n{segment['text'].strip()}\n\n" for seg_idx, segment in enumerate(result['segments']))
            with open(individual_srt_path, 'w', encoding='utf-8') as f: f.write(individual_srt_content)
            logger.info(f"å·²ä¸º '{video_file.name}' ç”Ÿæˆç‹¬ç«‹çš„SRTæ–‡ä»¶: {individual_srt_path.name}")
        if use_precise_subtitles and 'segments' in result:
            for segment in result['segments']:
                if 'words' in segment:
                    for word_info in segment['words']:
                        if 'start' in word_info and 'end' in word_info:
                            adjusted_word_info = word_info.copy()
                            adjusted_word_info['start'] += total_offset_seconds
                            adjusted_word_info['end'] += total_offset_seconds
                            all_word_segments.append(adjusted_word_info)
        for segment in result['segments']:
            start_str = Processor.seconds_to_time_str(segment['start'] + total_offset_seconds)
            end_str = Processor.seconds_to_time_str(segment['end'] + total_offset_seconds)
            combined_srt_content += f"{entry_index}\n{start_str} --> {end_str}\n{segment['text'].strip()}\n\n"
            entry_index += 1
        duration = Processor.get_video_duration(video_file)
        if duration > 0: total_offset_seconds += duration
        logger.info(f"è§†é¢‘ '{video_file.name}' å¤„ç†å®Œæˆï¼Œæ—¶é•¿: {duration:.2f}sã€‚ç´¯è®¡æ—¶é•¿: {total_offset_seconds:.2f}sã€‚")
        current_progress = 5 + int(transcribe_progress_total_weight * (i + 1) / len(video_files))
        progress_callback(current_progress)
    with open(combined_srt_path, 'w', encoding='utf-8') as f: f.write(combined_srt_content)
    logger.info(f"æ‰€æœ‰è§†é¢‘çš„SRTå·²åˆå¹¶åˆ°: {combined_srt_path.name}")
    if use_precise_subtitles:
        combined_word_segments_path = temp_dir / f"{combined_srt_path.stem}_word_segments.pkl"
        with open(combined_word_segments_path, 'wb') as f: pickle.dump(all_word_segments, f)
        logger.info(f"æ‰€æœ‰è§†é¢‘çš„è¯çº§åˆ«æ—¶é—´æˆ³æ•°æ®å·²åˆå¹¶åˆ°: {combined_word_segments_path.name}")

class Worker(QObject):
    progress = pyqtSignal(int)
    finished = pyqtSignal()
    error = pyqtSignal(str)
    log_received = pyqtSignal(str)
    def __init__(self, params: dict):
        super().__init__()
        self.params = params
        self.process = None
        self._is_running = True
    def run(self):
        queue = multiprocessing.Manager().Queue()
        self.process = multiprocessing.Process(target=run_processing_task, args=(queue, self.params))
        self.process.start()
        while self._is_running and (self.process.is_alive() or not queue.empty()):
            try:
                message_type, data = queue.get(timeout=0.1)
                if message_type == 'progress':
                    self.progress.emit(data)
                elif message_type == 'error':
                    self.error.emit(data)
                elif message_type == 'finished':
                    self.finished.emit()
                    self._is_running = False
                elif message_type == 'log':
                    self.log_received.emit(data)
            except Empty:
                continue
        if self.process:
            self.process.join()
    def stop(self):
        self._is_running = False
        if self.process and self.process.is_alive():
            self.process.terminate()
            self.process.join()

class DurationCalculator(QObject):
    duration_calculated = pyqtSignal(float)
    def __init__(self, path_str):
        super().__init__()
        self.path = Path(path_str)
    def run(self):
        total_duration = 0.0
        try:
            video_files = _get_video_files(self.path)
            for video_file in video_files:
                total_duration += Processor.get_video_duration(video_file)
        except Exception as e:
            logger.error(f"è®¡ç®—æ€»æ—¶é•¿æ—¶å‡ºé”™: {e}")
            total_duration = 0.0
        self.duration_calculated.emit(total_duration)

class OrientationChecker(QObject):
    orientation_checked = pyqtSignal(bool, bool)

    def __init__(self, path_str):
        super().__init__()
        self.path = Path(path_str)

    def run(self):
        has_landscape = False
        has_portrait = False
        try:
            video_files = _get_video_files(self.path)
            if not video_files:
                self.orientation_checked.emit(False, False)
                return
            for video_file in video_files:
                try:
                    probe = ffmpeg.probe(str(video_file))
                    video_stream = next((s for s in probe['streams'] if s['codec_type'] == 'video'), None)
                    if video_stream:
                        width = video_stream.get('width', 0)
                        height = video_stream.get('height', 0)
                        if width > height:
                            has_landscape = True
                            logger.info(f"æ£€æµ‹åˆ°æ¨ªå±è§†é¢‘: {video_file.name}")
                        else:
                            has_portrait = True
                            logger.info(f"æ£€æµ‹åˆ°ç«–å±æˆ–æ–¹å½¢è§†é¢‘: {video_file.name}")
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥è§†é¢‘ '{video_file.name}' æ–¹å‘æ—¶å‡ºé”™: {e}")
                    continue
                if has_landscape and has_portrait:
                    break
        except Exception as e:
            logger.error(f"æ£€æµ‹è§†é¢‘æ–¹å‘æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        self.orientation_checked.emit(has_landscape, has_portrait)

class MainWindow(QMainWindow):
    log_signal = pyqtSignal(str)

    def __init__(self):
        super().__init__()
        self.thread, self.worker = None, None
        self.duration_thread = None
        self.orientation_check_thread = None
        self.total_video_duration_seconds = 0
        self.config_path = "config.ini"
        self.config = configparser.ConfigParser()
        self.output_dir_name = "output_clips"
        self.supported_languages = {"ä¸­æ–‡": "Chinese", "è‹±æ–‡": "English", "æ—¥æ–‡": "Japanese", "è‡ªåŠ¨æ£€æµ‹": None}
        self.whisper_models = {
            "å¿«é€Ÿ (tiny)": "tiny",
            "åŸºç¡€ (base)": "base",
            "æ ‡å‡† (small)": "small",
            "ç²¾å‡† (medium)": "medium",
            "æœ€é«˜ç²¾åº¦ (large-v3)": "large-v3"
        }
        self.vcodecs_cpu = ['libx264', 'libx265', 'copy']
        self.vcodecs_gpu = ['h264_nvenc', 'hevc_nvenc']
        self.presets = {
            'libx264': ['veryslow', 'slow', 'medium', 'fast', 'veryfast', 'ultrafast'],
            'libx265': ['veryslow', 'slow', 'medium', 'fast', 'veryfast', 'ultrafast'],
            'h264_nvenc': ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7'],
            'hevc_nvenc': ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7'],
            'copy': []
        }
        self.has_gpu = False
        try:
            pynvml.nvmlInit()
            if pynvml.nvmlDeviceGetCount() > 0:
                self.has_gpu = True
                logger.info("æ£€æµ‹åˆ° NVIDIA GPUï¼Œå°†å¯ç”¨GPUåŠ é€Ÿé€‰é¡¹ã€‚")
            else:
                logger.warning("æœªæ£€æµ‹åˆ° NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œç¼–ç ã€‚")
        except pynvml.NVMLError:
            logger.warning("pynvml åˆå§‹åŒ–å¤±è´¥ã€‚æ— æ³•æ£€æµ‹NVIDIA GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œç¼–ç ã€‚")
        self.initUI()
        self.load_config()
        self.log_signal.connect(self.append_log)
        self.setup_logging()

    def initUI(self):
        self.setWindowTitle(f"{Config.APP_NAME} v{Config.APP_VERSION}")
        self.setGeometry(100, 100, 1200, 800)
        if resources:
            self.setWindowIcon(QIcon(":/logo.png"))
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_horizontal_layout = QHBoxLayout(central_widget)
        left_column_layout = QVBoxLayout()
        config_group = QGroupBox("æ ¸å¿ƒé…ç½®")
        api_layout = QFormLayout()
        self.input_path_edit = QLineEdit()
        self.input_path_edit.setReadOnly(True)
        self.input_path_edit.setPlaceholderText("è¯·é€‰æ‹©è¦å¤„ç†çš„è§†é¢‘æ–‡ä»¶æˆ–ç›®å½•...")
        self.browse_button = QPushButton("æµè§ˆ...")
        self.browse_button.clicked.connect(self.browse_input)
        input_layout = QHBoxLayout()
        input_layout.addWidget(self.input_path_edit)
        input_layout.addWidget(self.browse_button)
        api_layout.addRow(input_layout)
        self.api_key_edit = QLineEdit()
        self.api_key_edit.setEchoMode(QLineEdit.Password)
        self.language_combo = QComboBox()
        self.language_combo.addItems(self.supported_languages.keys())
        self.whisper_model_combo = QComboBox()
        self.whisper_model_combo.addItems(self.whisper_models.keys())
        api_layout.addRow("åƒé—®ï¼ˆé€šä¹‰ï¼‰ API Key:", self.api_key_edit)
        api_layout.addRow("è¯­éŸ³è¯†åˆ«è¯­è¨€:", self.language_combo)
        api_layout.addRow("è¯­éŸ³è¯†åˆ«æ¨¡å‹:", self.whisper_model_combo)
        config_group.setLayout(api_layout)
        left_column_layout.addWidget(config_group)
        param_group = QGroupBox("å‰ªè¾‘å‚æ•°")
        param_layout = QFormLayout()
        self.num_clips_spinbox = QSpinBox()
        self.num_clips_spinbox.setRange(1, 999)
        self.num_clips_spinbox.setValue(10)
        self.num_clips_spinbox.valueChanged.connect(self.update_recommendation_text)
        self.clip_duration_spinbox = QSpinBox()
        self.clip_duration_spinbox.setRange(10, 600)
        self.clip_duration_spinbox.setValue(30)
        self.clip_duration_spinbox.setSuffix(" ç§’")
        self.clip_duration_spinbox.valueChanged.connect(self.update_recommendation_text)
        self.max_tasks_spinbox = QSpinBox()
        self.max_tasks_spinbox.setRange(1, 20)
        self.max_tasks_spinbox.setValue(10)
        self.power_words_edit = QLineEdit()
        self.power_words_edit.setPlaceholderText("å¤šä¸ªè¯ç”¨é€—å·æˆ–ç©ºæ ¼éš”å¼€ï¼Œç•™ç©ºåˆ™ä¸ä½¿ç”¨å…³é”®è¯æ’åº")
        default_keywords = "è´¢æŠ¥,åˆ©æ¶¦,ä¼°å€¼,é£é™©,ç­–ç•¥,é€»è¾‘,å†…å¹•,è‚¡ä»·,å¸‚åœº,æŠ•èµ„,äº¤æ˜“,ä¸»åŠ›,èµ„æœ¬,æ æ†,é¢„æœŸ,åšå¼ˆ"
        self.power_words_edit.setText(default_keywords)
        self.subtitle_lines_spinbox = QSpinBox()
        self.subtitle_lines_spinbox.setRange(1, 3)
        self.subtitle_lines_spinbox.setValue(2)
        self.max_chars_per_line_spinbox = QSpinBox()
        self.max_chars_per_line_spinbox.setRange(5, 30)
        self.max_chars_per_line_spinbox.setValue(10)
        param_layout.addRow("ç”Ÿæˆç‰‡æ®µæ•°é‡:", self.num_clips_spinbox)
        param_layout.addRow("ç›®æ ‡ç‰‡æ®µæ—¶é•¿:", self.clip_duration_spinbox)
        self.recommendation_label = QLabel("è¯·å…ˆé€‰æ‹©è§†é¢‘ä»¥è·å–å»ºè®®ã€‚")
        self.recommendation_label.setObjectName("RecommendationLabel")
        self.recommendation_label.setWordWrap(True)
        param_layout.addRow("æ™ºèƒ½å»ºè®®:", self.recommendation_label)
        param_layout.addRow("é«˜å…‰å…³é”®è¯:", self.power_words_edit)
        param_layout.addRow("å­—å¹•æœ€å¤§è¡Œæ•°:", self.subtitle_lines_spinbox)
        param_layout.addRow("å­—å¹•æ¯è¡Œæœ€å¤§å­—æ•°:", self.max_chars_per_line_spinbox)
        param_layout.addRow("æœ€å¤§å¹¶å‘æ•°:", self.max_tasks_spinbox)
        self.landscape_to_portrait_checkbox = QCheckBox("æ¨ªå±è½¬ç«–å±")
        self.landscape_to_portrait_checkbox.setToolTip("å¦‚æœæ£€æµ‹åˆ°æ¨ªå±è§†é¢‘ï¼Œå‹¾é€‰æ­¤é¡¹ä¼šå°†å…¶å¼ºåˆ¶è½¬ä¸º9:16ç«–å±ï¼ˆä¸Šä¸‹åŠ é»‘è¾¹ï¼‰ã€‚\nä¸å‹¾é€‰åˆ™ä¿æŒåŸå§‹å®½é«˜æ¯”ï¼Œå¹¶å°†æ ‡é¢˜å’Œå­—å¹•ç›´æ¥çƒ§å½•åœ¨è§†é¢‘ç”»é¢ä¸Šã€‚")
        self.landscape_to_portrait_checkbox.setChecked(False)
        self.landscape_to_portrait_checkbox.setEnabled(False)
        self.burn_subtitle_checkbox = QCheckBox("æ·»åŠ æ ‡é¢˜å’Œå­—å¹•")
        self.precise_subtitle_checkbox = QCheckBox("å¯ç”¨ç²¾å‡†å­—å¹•")
        self.precise_subtitle_checkbox.setToolTip("æ¨èå¼€å¯ã€‚ä½¿ç”¨è¯çº§åˆ«æ—¶é—´æˆ³ï¼Œå£æ’­åŒæ­¥æ›´ç²¾å‡†ï¼Œä½†é¦–æ¬¡è¯†åˆ«ç¨æ…¢ã€‚")
        self.add_bgm_checkbox = QCheckBox("æ·»åŠ éšæœºBGM")
        self.gpu_accel_checkbox = QCheckBox("å¯ç”¨GPUåŠ é€Ÿ (NVENC)")
        self.gpu_accel_checkbox.setEnabled(self.has_gpu)
        self.gpu_accel_checkbox.setChecked(self.has_gpu)
        self.gpu_accel_checkbox.stateChanged.connect(self.update_vcodec_options)
        checkbox_layout = QHBoxLayout()
        checkbox_layout.addWidget(self.landscape_to_portrait_checkbox)
        checkbox_layout.addWidget(self.burn_subtitle_checkbox)
        checkbox_layout.addWidget(self.precise_subtitle_checkbox)
        checkbox_layout.addWidget(self.add_bgm_checkbox)
        checkbox_layout.addWidget(self.gpu_accel_checkbox)
        checkbox_layout.addStretch()
        param_layout.addRow(checkbox_layout)
        param_group.setLayout(param_layout)
        left_column_layout.addWidget(param_group)
        left_column_layout.addStretch(1)
        bottom_controls_layout = QVBoxLayout()
        action_button_layout = QHBoxLayout()
        self.start_button = QPushButton("ğŸš€ å¼€å§‹ç”Ÿæˆ")
        self.start_button.setObjectName("PrimaryButton")
        self.start_button.setFixedHeight(45)
        self.start_button.clicked.connect(self.start_processing)
        self.stop_button = QPushButton("ğŸ›‘ åœæ­¢")
        self.stop_button.setFixedHeight(45)
        self.stop_button.setEnabled(False)
        self.stop_button.clicked.connect(self.stop_processing)
        self.open_dir_button = QPushButton("ğŸ“‚ æ‰“å¼€ç‰‡æ®µç›®å½•")
        self.open_dir_button.setFixedHeight(45)
        self.open_dir_button.clicked.connect(self.open_output_directory)
        action_button_layout.addWidget(self.start_button)
        action_button_layout.addWidget(self.stop_button)
        action_button_layout.addWidget(self.open_dir_button)
        bottom_controls_layout.addLayout(action_button_layout)
        self.progress_bar = QProgressBar()
        self.progress_bar.setValue(0)
        self.progress_bar.setTextVisible(False)
        bottom_controls_layout.addWidget(self.progress_bar)
        left_column_layout.addLayout(bottom_controls_layout)
        right_column_layout = QVBoxLayout()
        encoding_group = QGroupBox("é«˜çº§ç¼–ç è®¾ç½®")
        encoding_form_layout = QFormLayout()
        self.help_button = QPushButton("?")
        self.help_button.setFixedSize(24, 24)
        self.help_button.setStyleSheet("font-weight: bold; border-radius: 12px;")
        self.help_button.setToolTip("ç‚¹å‡»æŸ¥çœ‹å…³äºç¼–ç è®¾ç½®çš„è¯¦ç»†è¯´æ˜å’Œæ¨è")
        self.help_button.clicked.connect(self.show_encoding_help)
        self.vcodec_combo = QComboBox()
        self.preset_combo = QComboBox()
        self.crf_spinbox = QSpinBox()
        self.crf_spinbox.setRange(0, 51)
        self.crf_spinbox.setValue(18)
        self.vcodec_combo.currentTextChanged.connect(self.update_preset_options)
        vcodec_label = QLabel("è§†é¢‘ç¼–ç å™¨:")
        preset_label = QLabel("ç¼–ç é¢„è®¾:")
        crf_label = QLabel("è´¨é‡å› å­ (CRF/QP):")
        help_label = QLabel("å‚æ•°è¯´æ˜ï¼š")
        encoding_form_layout.addRow(help_label, self.help_button)
        encoding_form_layout.addRow(vcodec_label, self.vcodec_combo)
        encoding_form_layout.addRow(preset_label, self.preset_combo)
        encoding_form_layout.addRow(crf_label, self.crf_spinbox)
        encoding_group.setLayout(encoding_form_layout)
        right_column_layout.addWidget(encoding_group)
        self.update_vcodec_options()
        log_group = QGroupBox("è¿è¡Œæ—¥å¿—")
        log_layout = QVBoxLayout()
        self.log_output = QTextEdit()
        self.log_output.setReadOnly(True)
        self.log_output.setPlaceholderText("è¿è¡Œæ—¥å¿—å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...")
        log_layout.addWidget(self.log_output)
        log_group.setLayout(log_layout)
        right_column_layout.addWidget(log_group)
        left_widget = QWidget()
        left_widget.setLayout(left_column_layout)
        right_widget = QWidget()
        right_widget.setLayout(right_column_layout)
        main_horizontal_layout.addWidget(left_widget, 1)
        main_horizontal_layout.addWidget(right_widget, 1)

    def update_vcodec_options(self):
        current_vcodec = self.vcodec_combo.currentText()
        self.vcodec_combo.clear()
        if self.gpu_accel_checkbox.isChecked() and self.has_gpu:
            self.vcodec_combo.addItems(self.vcodecs_gpu)
            if current_vcodec in self.vcodecs_gpu:
                self.vcodec_combo.setCurrentText(current_vcodec)
            else:
                self.vcodec_combo.setCurrentText(self.vcodecs_gpu[0])
        else:
            self.vcodec_combo.addItems(self.vcodecs_cpu)
            if current_vcodec in self.vcodecs_cpu:
                self.vcodec_combo.setCurrentText(current_vcodec)
            else:
                self.vcodec_combo.setCurrentText(self.vcodecs_cpu[0])
        new_vcodec = self.vcodec_combo.currentText()
        self.preset_combo.clear()
        presets = self.presets.get(new_vcodec, [])
        self.preset_combo.addItems(presets)
        if new_vcodec in self.vcodecs_gpu:
            if 'p4' in presets:
                self.preset_combo.setCurrentText('p4')
        elif new_vcodec in self.vcodecs_cpu:
            if 'medium' in presets:
                self.preset_combo.setCurrentText('medium')
        self.preset_combo.setEnabled(bool(presets))
        self.crf_spinbox.setEnabled(new_vcodec != 'copy')

    def update_preset_options(self, vcodec):
        current_preset = self.preset_combo.currentText()
        self.preset_combo.clear()
        presets = self.presets.get(vcodec, [])
        self.preset_combo.addItems(presets)
        if current_preset in presets:
            self.preset_combo.setCurrentText(current_preset)
        self.preset_combo.setEnabled(bool(presets))
        self.crf_spinbox.setEnabled(vcodec != 'copy')

    def show_encoding_help(self):
        dialog = QDialog(self)
        dialog.setWindowTitle("ç¼–ç è®¾ç½®å¸®åŠ©")
        dialog.setMinimumSize(800, 500)
        layout = QVBoxLayout(dialog)
        text_edit = QTextEdit()
        text_edit.setReadOnly(True)
        help_text = """
        <!DOCTYPE html>
        <html>
        <head>
            <style>
                body { font-family: "Segoe UI", "Microsoft YaHei", sans-serif; background-color: #1A1D2A; color: #D0D0D0; }
                h2 { color: #25F4EE; border-bottom: 1px solid #4A4E60; padding-bottom: 5px; }
                p, li { line-height: 1.6; }
                b { color: #FE2C55; }
                code { background-color: #3B3F51; padding: 2px 6px; border-radius: 4px; font-family: "Consolas", "Courier New", monospace; }
                .recommendation { border-left: 3px solid #25F4EE; padding-left: 15px; margin-top: 10px; background-color: #24283B; }
                hr { border: none; border-top: 1px solid #3B3F51; }
            </style>
        </head>
        <body>
            <h2>é«˜çº§ç¼–ç è®¾ç½®è¯´æ˜</h2>
            <p>è¿™é‡Œæ˜¯è§†é¢‘è¾“å‡ºçš„â€œä¸“å®¶æ¨¡å¼â€ï¼Œå…è®¸æ‚¨ç²¾ç»†æ§åˆ¶æœ€ç»ˆè§†é¢‘çš„<b>ç”»è´¨</b>ã€<b>æ–‡ä»¶å¤§å°</b>å’Œ<b>ç”Ÿæˆé€Ÿåº¦</b>ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šå¦‚ä½•é€‰æ‹©ï¼Œ<b>ä½¿ç”¨é»˜è®¤å€¼æ˜¯å…¼é¡¾è´¨é‡å’Œé€Ÿåº¦çš„æœ€ä½³é€‰æ‹©</b>ã€‚</p>
            <hr>

            <h2>1. è§†é¢‘ç¼–ç å™¨ (Video Encoder)</h2>
            <p>ç¼–ç å™¨æ˜¯å†³å®šè§†é¢‘å¦‚ä½•è¢«å‹ç¼©çš„â€œå¼•æ“â€ã€‚ä¸åŒçš„å¼•æ“æœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼š</p>
            <ul>
                <li><b><code>libx264</code> (CPUç¼–ç , H.264)</b>
                    <ul>
                        <li><b>ç‰¹ç‚¹:</b> è´¨é‡æé«˜ï¼Œå…¼å®¹æ€§æœ€å¥½ã€‚å‡ ä¹æ‰€æœ‰è®¾å¤‡å’Œå¹³å°éƒ½èƒ½æµç•…æ’­æ”¾ã€‚</li>
                        <li><b>ç¼ºç‚¹:</b> å®Œå…¨ä¾èµ–CPUè¿›è¡Œè®¡ç®—ï¼Œé€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢ã€‚</li>
                        <li><b>é€‚ç”¨åœºæ™¯:</b> è¿½æ±‚æœ€é«˜ç”»è´¨å’Œæœ€ä½³å…¼å®¹æ€§ï¼Œä¸”ä¸ä»‹æ„ç­‰å¾…æ›´é•¿æ—¶é—´çš„ç”¨æˆ·ã€‚</li>
                    </ul>
                </li>
                <li><b><code>libx265</code> (CPUç¼–ç , H.265/HEVC)</b>
                    <ul>
                        <li><b>ç‰¹ç‚¹:</b> æ–°ä¸€ä»£ç¼–ç æ ‡å‡†ã€‚åœ¨è‚‰çœ¼å‡ ä¹çœ‹ä¸å‡ºå·®åˆ«çš„æƒ…å†µä¸‹ï¼Œæ–‡ä»¶å¤§å°æ¯” <code>libx264</code> å°çº¦ <b>30-50%</b>ã€‚</li>
                        <li><b>ç¼ºç‚¹:</b> ç¼–ç è¿‡ç¨‹æ¯” <code>libx264</code> æ›´æ…¢ã€‚éƒ¨åˆ†è¾ƒè€çš„è®¾å¤‡æˆ–æ’­æ”¾å™¨å¯èƒ½ä¸æ”¯æŒH.265æ ¼å¼ã€‚</li>
                        <li><b>é€‚ç”¨åœºæ™¯:</b> å¯¹å­˜å‚¨ç©ºé—´æ•æ„Ÿï¼Œå¸Œæœ›è·å¾—æ›´å°æ–‡ä»¶ä½“ç§¯ï¼Œä¸”ç¡®è®¤æ’­æ”¾è®¾å¤‡æ”¯æŒH.265çš„ç”¨æˆ·ã€‚</li>
                    </ul>
                </li>
                <li><b><code>h264_nvenc</code> (GPUåŠ é€Ÿ, H.264)</b>
                    <ul>
                        <li><b>ç‰¹ç‚¹:</b> åˆ©ç”¨NVIDIAæ˜¾å¡è¿›è¡Œç¡¬ä»¶ç¼–ç ï¼Œé€Ÿåº¦æ¯”CPUç¼–ç å¿« <b>5åˆ°10å€</b>ç”šè‡³æ›´å¤šï¼Œèƒ½æå¤§ç¼©çŸ­ç­‰å¾…æ—¶é—´ã€‚</li>
                        <li><b>ç¼ºç‚¹:</b> åœ¨åŒç­‰æ–‡ä»¶å¤§å°ä¸‹ï¼Œç”»è´¨é€šå¸¸ç•¥é€Šäº <code>libx264</code> çš„æ…¢é€Ÿé¢„è®¾ï¼Œä½†å¯¹äºçŸ­è§†é¢‘å’Œç¤¾äº¤åª’ä½“åˆ†äº«ï¼Œè¿™ç§å·®å¼‚å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚</li>
                        <li><b>é€‚ç”¨åœºæ™¯:</b> <b>å¼ºçƒˆæ¨èæ‹¥æœ‰NVIDIAæ˜¾å¡çš„ç”¨æˆ·ä½¿ç”¨ï¼</b>æ˜¯é€Ÿåº¦å’Œè´¨é‡çš„å®Œç¾ç»“åˆã€‚</li>
                    </ul>
                </li>
                <li><b><code>hevc_nvenc</code> (GPUåŠ é€Ÿ, H.265/HEVC)</b>
                    <ul>
                        <li><b>ç‰¹ç‚¹:</b> ç»“åˆäº†GPUçš„è¶…å¿«é€Ÿåº¦å’ŒH.265çš„é«˜å‹ç¼©ç‡ï¼Œèƒ½ç”Ÿæˆä½“ç§¯å¾ˆå°çš„è§†é¢‘æ–‡ä»¶ã€‚</li>
                        <li><b>ç¼ºç‚¹:</b> åŒ <code>libx265</code>ï¼Œéœ€è¦è€ƒè™‘æ’­æ”¾è®¾å¤‡çš„å…¼å®¹æ€§ã€‚</li>
                        <li><b>é€‚ç”¨åœºæ™¯:</b> å¸Œæœ›å¿«é€Ÿç”Ÿæˆè¶…å°ä½“ç§¯è§†é¢‘ï¼Œå¹¶ç¡®è®¤æ’­æ”¾è®¾å¤‡æ”¯æŒçš„ç”¨æˆ·ã€‚</li>
                    </ul>
                </li>
                 <li><b><code>copy</code> (æ— æŸå¤åˆ¶)</b>
                    <ul>
                        <li><b>ç‰¹ç‚¹:</b> <b>å®Œå…¨æ— æŸ</b>ã€‚å®ƒä¸è¿›è¡Œä»»ä½•é‡æ–°ç¼–ç ï¼Œåªæ˜¯å°†åŸå§‹è§†é¢‘æ•°æ®åŸå°ä¸åŠ¨åœ°â€œå‰ªåˆ‡â€å‡ºæ¥ã€‚é€Ÿåº¦æœ€å¿«ï¼Œç”»è´¨ä¸æºæ–‡ä»¶100%ç›¸åŒã€‚</li>
                        <li><b>é‡å¤§é™åˆ¶:</b> åœ¨æ­¤æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰éœ€è¦ä¿®æ”¹ç”»é¢çš„æ“ä½œï¼ˆåŒ…æ‹¬<b>æ·»åŠ æ ‡é¢˜ã€å­—å¹•ã€è½¬ä¸ºç«–å±çš„é»‘è¾¹</b>ï¼‰éƒ½å°†<b>å®Œå…¨å¤±æ•ˆ</b>ã€‚</li>
                        <li><b>é€‚ç”¨åœºæ™¯:</b> ä»…å½“æ‚¨éœ€è¦å¯¹åŸå§‹è§†é¢‘è¿›è¡Œç²¾ç¡®è£å‰ªï¼Œä¸”ä¸éœ€è¦ä»»ä½•ç”»é¢ä¿®æ”¹æ—¶ä½¿ç”¨ã€‚<b>å¯¹äºæœ¬å·¥å…·çš„æ ¸å¿ƒåŠŸèƒ½ï¼ˆç”Ÿæˆå¸¦æ ‡é¢˜å­—å¹•çš„ç«–å±è§†é¢‘ï¼‰ï¼Œæ­¤é€‰é¡¹æ— æ•ˆã€‚</b></li>
                    </ul>
                </li>
            </ul>
            <div class="recommendation">
                <p><b>å¦‚ä½•é€‰æ‹©ï¼Ÿ</b><br>
                - æœ‰NVIDIAæ˜¾å¡ï¼Ÿ <b>é¦–é€‰ <code>h264_nvenc</code></b>ã€‚<br>
                - æ²¡æœ‰æ˜¾å¡ä½†è¿½æ±‚ç”»è´¨ï¼Ÿ <b>é€‰æ‹© <code>libx264</code></b>ã€‚<br>
                - å­˜å‚¨ç©ºé—´æå…¶å®è´µï¼Ÿ <b>å¯ä»¥å°è¯• <code>libx265</code> æˆ– <code>hevc_nvenc</code></b>ã€‚
                </p>
            </div>
            <hr>

            <h2>2. ç¼–ç é¢„è®¾ (Encoding Preset)</h2>
            <p>é¢„è®¾æ˜¯æ‚¨å‘Šè¯‰ç¼–ç å™¨â€œå¯ä»¥èŠ±å¤šå°‘æ—¶é—´æ¥æ€è€ƒå¦‚ä½•å‹ç¼©å¾—æ›´å¥½â€ã€‚è¶Šæ…¢çš„é¢„è®¾ï¼Œç¼–ç å™¨åˆ†æå¾—è¶Šç²¾ç»†ï¼Œå‹ç¼©æ•ˆæœè¶Šå¥½ï¼ˆåŒç­‰ç”»è´¨ä¸‹æ–‡ä»¶æ›´å°ï¼‰ã€‚</p>
            <ul>
                <li><b>å¯¹äºCPUç¼–ç  (<code>libx264</code> / <code>libx265</code>):</b> èŒƒå›´ä» <code>veryslow</code> åˆ° <code>ultrafast</code>ã€‚
                    <ul>
                        <li><code>veryslow</code>: é€Ÿåº¦æœ€æ…¢ï¼Œå‹ç¼©ç‡æœ€é«˜ã€‚</li>
                        <li><code>medium</code>: é€Ÿåº¦å’Œå‹ç¼©ç‡çš„å®˜æ–¹æ¨èå¹³è¡¡ç‚¹ã€‚</li>
                        <li><code>ultrafast</code>: é€Ÿåº¦æœ€å¿«ï¼Œä½†æ–‡ä»¶ä¼šå¤§å¾ˆå¤šã€‚</li>
                    </ul>
                </li>
                <li><b>å¯¹äºGPUç¼–ç  (<code>..._nvenc</code>):</b> èŒƒå›´ä» <code>p1</code> (æ…¢) åˆ° <code>p7</code> (å¿«)ã€‚
                    <ul>
                        <li><code>p1</code>-<code>p3</code> (æ…¢é€Ÿ): è´¨é‡æ›´é«˜ã€‚</li>
                        <li><code>p4</code> (é»˜è®¤): è‰¯å¥½çš„å¹³è¡¡ã€‚</li>
                        <li><code>p5</code>-<code>p7</code> (å¿«é€Ÿ): é€Ÿåº¦æ›´å¿«ï¼Œè´¨é‡ç•¥æœ‰ä¸‹é™ã€‚</li>
                    </ul>
                </li>
            </ul>
             <div class="recommendation">
                <p><b>å¦‚ä½•é€‰æ‹©ï¼Ÿ</b><br>
                - é€šå¸¸ä¿æŒé»˜è®¤çš„ <code>medium</code> æˆ– <code>p4</code> å³å¯ã€‚<br>
                - å¦‚æœå‘ç°æ–‡ä»¶æœ‰ç‚¹å¤§ï¼Œå¯ä»¥å°è¯•æ›´æ…¢ä¸€æ¡£çš„é¢„è®¾ï¼ˆå¦‚ <code>slow</code> æˆ– <code>p3</code>ï¼‰ï¼Œè¿™ä¼šåœ¨ä¸ç‰ºç‰²ç”»è´¨çš„æƒ…å†µä¸‹å‡å°æ–‡ä»¶ä½“ç§¯ã€‚
                </p>
            </div>
            <hr>

            <h2>3. è´¨é‡å› å­ (CRF / QP)</h2>
            <p>è¿™æ˜¯æ§åˆ¶è§†é¢‘æœ€ç»ˆè§†è§‰è´¨é‡çš„æœ€ç›´æ¥å‚æ•°ã€‚<b>è¯·è®°ä½ä¸€ä¸ªæ ¸å¿ƒåŸåˆ™ï¼šæ•°å€¼è¶Šä½ï¼Œç”»è´¨è¶Šé«˜ï¼Œæ–‡ä»¶è¶Šå¤§ã€‚</b></p>
            <ul>
                <li><b>CRF (æ’å®šé€Ÿç‡å› å­) - ç”¨äºCPUç¼–ç  (<code>libx264</code> / <code>libx265</code>)</b>
                    <p>CRFæ¨¡å¼ä¼šå°½åŠ›åœ¨æ•´ä¸ªè§†é¢‘ä¸­ä¿æŒä¸€ä¸ªæ’å®šçš„â€œæ„ŸçŸ¥è´¨é‡â€ã€‚</p>
                    <ul>
                        <li><code>0</code>: æ•°å­¦ä¸Šçš„æ— æŸå‹ç¼©ã€‚æ–‡ä»¶ä½“ç§¯ä¼š<b>æå…¶å·¨å¤§</b>ï¼Œé€šå¸¸ä»…ç”¨äºä¸“ä¸šå­˜æ¡£ï¼Œä¸æ¨èæ—¥å¸¸ä½¿ç”¨ã€‚</li>
                        <li><code>16-18</code>: <b>è§†è§‰æ— æŸèŒƒå›´ã€‚</b>äººçœ¼å‡ ä¹æ— æ³•åˆ†è¾¨ä¸æºè§†é¢‘çš„å·®å¼‚ï¼Œæ˜¯è¿½æ±‚é«˜è´¨é‡è¾“å‡ºçš„ç†æƒ³é€‰æ‹©ã€‚</li>
                        <li><code>23</code>: <b>é«˜è´¨é‡é»˜è®¤å€¼ã€‚</b> è´¨é‡éå¸¸å¥½ï¼Œä¸”æ–‡ä»¶å¤§å°é€‚ä¸­ã€‚</li>
                        <li><code>28</code> åŠä»¥ä¸Š: ç”»è´¨å¼€å§‹å‡ºç°è‚‰çœ¼å¯è§çš„æŸå¤±ã€‚</li>
                    </ul>
                </li>
                <li><b>QP (é‡åŒ–å‚æ•°) - ç”¨äºGPUç¼–ç  (<code>..._nvenc</code>)</b>
                    <p>QPæ¨¡å¼è¯•å›¾ä¸ºè§†é¢‘çš„æ¯ä¸ªéƒ¨åˆ†åº”ç”¨ä¸€ä¸ªå›ºå®šçš„é‡åŒ–çº§åˆ«ï¼Œæ¦‚å¿µä¸Šä¸CRFç±»ä¼¼ã€‚</p>
                     <ul>
                        <li><code>0</code>: åŒæ ·æ˜¯æ— æŸï¼ŒåŒæ ·ä¸æ¨èã€‚</li>
                        <li><code>18-22</code>: <b>éå¸¸é«˜çš„è´¨é‡èŒƒå›´ã€‚</b></li>
                        <li><code>23-25</code>: <b>é«˜è´¨é‡èŒƒå›´</b>ï¼Œé€šå¸¸æ˜¯å¾ˆå¥½çš„å¹³è¡¡ç‚¹ã€‚</li>
                        <li><code>28</code> åŠä»¥ä¸Š: ç”»è´¨ä¸‹é™ä¼šæ¯”è¾ƒæ˜æ˜¾ã€‚</li>
                    </ul>
                </li>
            </ul>
            <div class="recommendation">
                <p><b>å¦‚ä½•é€‰æ‹©ï¼Ÿ</b><br>
                - æƒ³è¦æœ€å¥½çš„ç”»è´¨ï¼Ÿè®¾ç½® <b>18</b> æˆ–æ›´ä½ã€‚<br>
                - æƒ³è¦åœ¨è´¨é‡å’Œæ–‡ä»¶å¤§å°é—´å–å¾—å®Œç¾å¹³è¡¡ï¼Ÿä½¿ç”¨é»˜è®¤å€¼æˆ–åœ¨ <b>20-23</b> ä¹‹é—´é€‰æ‹©ã€‚<br>
                - æƒ³å¿«é€Ÿç”Ÿæˆä¸€ä¸ªé¢„è§ˆç‰ˆçš„å°æ ·ï¼Ÿå¯ä»¥è®¾ç½®ä¸º <b>26-28</b>ã€‚
                </p>
            </div>
        </body>
        </html>
        """
        text_edit.setHtml(help_text)
        button_box = QDialogButtonBox(QDialogButtonBox.Ok)
        button_box.accepted.connect(dialog.accept)
        layout.addWidget(text_edit)
        layout.addWidget(button_box)
        dialog.exec_()

    def setup_logging(self):
        if any(isinstance(h, PyQtLogHandler) for h in logger.handlers):
            return
        handler = PyQtLogHandler(self.log_signal)
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    def load_config(self):
        self.config.read(self.config_path, encoding='utf-8')
        self.api_key_edit.setText(self.config.get('api', 'api_key', fallback=''))
        self.num_clips_spinbox.setValue(self.config.getint('settings', 'num_clips', fallback=10))
        self.clip_duration_spinbox.setValue(self.config.getint('settings', 'clip_duration', fallback=30))
        self.max_tasks_spinbox.setValue(self.config.getint('settings', 'max_concurrent_tasks', fallback=10))
        self.subtitle_lines_spinbox.setValue(self.config.getint('settings', 'subtitle_lines', fallback=2))
        self.max_chars_per_line_spinbox.setValue(self.config.getint('settings', 'max_chars_per_line', fallback=10))
        last_lang = self.config.get('settings', 'language', fallback='ä¸­æ–‡')
        self.language_combo.setCurrentText(last_lang)
        self.power_words_edit.setText(self.config.get('settings', 'power_words', fallback="è´¢æŠ¥,åˆ©æ¶¦,ä¼°å€¼,é£é™©,ç­–ç•¥,é€»è¾‘,å†…å¹•,è‚¡ä»·,å¸‚åœº,æŠ•èµ„,äº¤æ˜“,ä¸»åŠ›,èµ„æœ¬,æ æ†,é¢„æœŸ,åšå¼ˆ"))
        saved_model_name = self.config.get('settings', 'whisper_model', fallback='medium')
        display_name_to_set = "ç²¾å‡† (medium)"
        for display_name, model_name in self.whisper_models.items():
            if model_name == saved_model_name:
                display_name_to_set = display_name
                break
        self.whisper_model_combo.setCurrentText(display_name_to_set)
        self.burn_subtitle_checkbox.setChecked(self.config.getboolean('settings', 'burn_subtitles', fallback=False))
        self.precise_subtitle_checkbox.setChecked(self.config.getboolean('settings', 'precise_subtitles', fallback=True))
        self.add_bgm_checkbox.setChecked(self.config.getboolean('settings', 'add_bgm', fallback=False))
        use_gpu = self.config.getboolean('settings', 'use_gpu', fallback=self.has_gpu)
        self.gpu_accel_checkbox.setChecked(use_gpu and self.has_gpu)
        self.update_vcodec_options()
        default_vcodec = self.vcodec_combo.itemText(0) if self.vcodec_combo.count() > 0 else 'libx264'
        self.vcodec_combo.setCurrentText(self.config.get('encoding', 'vcodec', fallback=default_vcodec))
        self.update_preset_options(self.vcodec_combo.currentText())
        default_preset = 'p4' if self.vcodec_combo.currentText() in self.vcodecs_gpu else 'medium'
        self.preset_combo.setCurrentText(self.config.get('encoding', 'preset', fallback=default_preset))
        self.crf_spinbox.setValue(self.config.getint('encoding', 'crf', fallback=18))
        logger.info(f"å·²ä» {self.config_path} åŠ è½½é…ç½®ã€‚")

    def save_config(self):
        if not self.config.has_section('api'): self.config.add_section('api')
        if not self.config.has_section('settings'): self.config.add_section('settings')
        if not self.config.has_section('encoding'): self.config.add_section('encoding')
        self.config.set('api', 'api_key', self.api_key_edit.text())
        self.config.set('api', 'provider', 'dashscope')
        self.config.set('api', 'model_name', 'qwen-plus-2025-07-28')
        self.config.set('settings', 'num_clips', str(self.num_clips_spinbox.value()))
        self.config.set('settings', 'clip_duration', str(self.clip_duration_spinbox.value()))
        self.config.set('settings', 'max_concurrent_tasks', str(self.max_tasks_spinbox.value()))
        self.config.set('settings', 'subtitle_lines', str(self.subtitle_lines_spinbox.value()))
        self.config.set('settings', 'max_chars_per_line', str(self.max_chars_per_line_spinbox.value()))
        self.config.set('settings', 'language', self.language_combo.currentText())
        self.config.set('settings', 'power_words', self.power_words_edit.text())
        selected_model_display = self.whisper_model_combo.currentText()
        model_name = self.whisper_models.get(selected_model_display, 'medium')
        self.config.set('settings', 'whisper_model', model_name)
        self.config.set('settings', 'burn_subtitles', str(self.burn_subtitle_checkbox.isChecked()))
        self.config.set('settings', 'precise_subtitles', str(self.precise_subtitle_checkbox.isChecked()))
        self.config.set('settings', 'add_bgm', str(self.add_bgm_checkbox.isChecked()))
        self.config.set('settings', 'use_gpu', str(self.gpu_accel_checkbox.isChecked()))
        self.config.set('encoding', 'vcodec', self.vcodec_combo.currentText())
        self.config.set('encoding', 'preset', self.preset_combo.currentText())
        self.config.set('encoding', 'crf', str(self.crf_spinbox.value()))
        with open(self.config_path, 'w', encoding='utf-8') as configfile:
            self.config.write(configfile)
        logger.info(f"é…ç½®å·²ä¿å­˜åˆ° {self.config_path}ã€‚")

    def browse_input(self):
        choice_box = QMessageBox(self)
        choice_box.setWindowTitle("é€‰æ‹©è¾“å…¥ç±»å‹")
        choice_box.setText("æ‚¨å¸Œæœ›å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œè¿˜æ˜¯åŒ…å«å¤šä¸ªè§†é¢‘çš„æ•´ä¸ªç›®å½•ï¼Ÿ")
        choice_box.setStyleSheet(STYLESHEET)
        file_button = choice_box.addButton("é€‰æ‹©å•ä¸ªæ–‡ä»¶ (ç”µå½±)", QMessageBox.ActionRole)
        dir_button = choice_box.addButton("é€‰æ‹©æ•´ä¸ªç›®å½• (è¿ç»­å‰§)", QMessageBox.ActionRole)
        cancel_button = choice_box.addButton("å–æ¶ˆ", QMessageBox.RejectRole)
        choice_box.setDefaultButton(dir_button)
        choice_box.exec_()
        path = ""
        clicked = choice_box.clickedButton()
        if clicked == file_button:
            video_extensions = " ".join([f"*{ext}" for ext in ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv']])
            file_path_tuple = QFileDialog.getOpenFileName(self, "é€‰æ‹©å•ä¸ªè§†é¢‘æ–‡ä»¶", "", f"è§†é¢‘æ–‡ä»¶ ({video_extensions});;æ‰€æœ‰æ–‡ä»¶ (*)")
            if file_path_tuple and file_path_tuple[0]:
                path = file_path_tuple[0]
        elif clicked == dir_button:
            dir_path = QFileDialog.getExistingDirectory(self, "é€‰æ‹©åŒ…å«è§†é¢‘çš„ç›®å½•")
            if dir_path:
                path = dir_path
        self.landscape_to_portrait_checkbox.setEnabled(False)
        self.landscape_to_portrait_checkbox.setChecked(True)
        if path:
            self.input_path_edit.setText(path)
            logger.info(f"å·²é€‰æ‹©è¾“å…¥è·¯å¾„: {path}")
            self.calculate_total_duration(path)
            self.check_video_orientation(path)
        elif clicked != cancel_button:
            logger.info("ç”¨æˆ·å–æ¶ˆäº†æ–‡ä»¶/ç›®å½•é€‰æ‹©ã€‚")

    def check_video_orientation(self, path_str):
        logger.info("æ­£åœ¨æ£€æµ‹è§†é¢‘æ–¹å‘...")
        if hasattr(self, 'orientation_check_thread') and self.orientation_check_thread and self.orientation_check_thread.isRunning():
            self.orientation_check_thread.quit()
            self.orientation_check_thread.wait()
        self.orientation_check_thread = QThread()
        self.orientation_checker = OrientationChecker(path_str)
        self.orientation_checker.moveToThread(self.orientation_check_thread)
        self.orientation_checker.orientation_checked.connect(self.on_orientation_checked)
        self.orientation_check_thread.started.connect(self.orientation_checker.run)
        self.orientation_check_thread.finished.connect(self.orientation_checker.deleteLater)
        self.orientation_check_thread.finished.connect(self.orientation_check_thread.deleteLater)
        self.orientation_check_thread.start()

    def on_orientation_checked(self, has_landscape: bool, has_portrait: bool):
        if has_landscape and not has_portrait:
            logger.info("æ£€æµ‹åˆ°è¾“å…¥ç´ æå…¨ä¸ºæ¨ªå±è§†é¢‘ã€‚ç”¨æˆ·ç°åœ¨å¯ä»¥é€‰æ‹©æ˜¯å¦è¿›è¡Œè½¬æ¢ã€‚")
            self.landscape_to_portrait_checkbox.setEnabled(True)
            self.landscape_to_portrait_checkbox.setChecked(False)
        elif has_portrait and not has_landscape:
            logger.info("æ£€æµ‹åˆ°è¾“å…¥ç´ æå…¨ä¸ºç«–å±è§†é¢‘ã€‚å°†ç¦ç”¨æ¨ªå±è½¬æ¢é€‰é¡¹ã€‚")
            self.landscape_to_portrait_checkbox.setEnabled(False)
            self.landscape_to_portrait_checkbox.setChecked(False)
        elif has_landscape and has_portrait:
            logger.warning("æ£€æµ‹åˆ°è¾“å…¥ç´ æåŒ…å«æ¨ªå±å’Œç«–å±æ··åˆè§†é¢‘ã€‚ä¸ºä¿è¯å¤„ç†ä¸€è‡´æ€§ï¼Œå°†ç¦ç”¨æ¨ªå±è½¬æ¢é€‰é¡¹å¹¶æŒ‰é»˜è®¤é€»è¾‘å¤„ç†ã€‚")
            self.landscape_to_portrait_checkbox.setEnabled(False)
            self.landscape_to_portrait_checkbox.setChecked(False)
        else:
            logger.info("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè§†é¢‘æˆ–æ— æ³•ç¡®å®šè§†é¢‘æ–¹å‘ã€‚å°†ç¦ç”¨æ¨ªå±è½¬æ¢é€‰é¡¹ã€‚")
            self.landscape_to_portrait_checkbox.setEnabled(False)
            self.landscape_to_portrait_checkbox.setChecked(False)
        if hasattr(self, 'orientation_check_thread') and self.orientation_check_thread:
            self.orientation_check_thread.quit()
            self.orientation_check_thread.wait()
            self.orientation_check_thread = None
            self.orientation_checker = None

    def calculate_total_duration(self, path_str):
        self.recommendation_label.setText("æ­£åœ¨è®¡ç®—è§†é¢‘æ€»æ—¶é•¿...")
        if hasattr(self, 'duration_thread') and self.duration_thread and self.duration_thread.isRunning():
            self.duration_thread.quit()
            self.duration_thread.wait()
        self.duration_thread = QThread()
        self.duration_worker = DurationCalculator(path_str)
        self.duration_worker.moveToThread(self.duration_thread)
        self.duration_worker.duration_calculated.connect(self.on_duration_calculated)
        self.duration_thread.started.connect(self.duration_worker.run)
        self.duration_thread.finished.connect(self.duration_worker.deleteLater)
        self.duration_thread.finished.connect(self.duration_thread.deleteLater)
        self.duration_thread.start()

    def on_duration_calculated(self, total_seconds):
        self.total_video_duration_seconds = total_seconds
        self.update_recommendation_text()

    def update_recommendation_text(self):
        if self.total_video_duration_seconds <= 0:
            self.recommendation_label.setText("è¯·å…ˆé€‰æ‹©è§†é¢‘ä»¥è·å–å»ºè®®ã€‚")
            return
        num_clips = self.num_clips_spinbox.value()
        clip_duration = self.clip_duration_spinbox.value()
        total_minutes = self.total_video_duration_seconds / 60
        if clip_duration == 0: return
        low_density_ratio = 0.20
        high_density_ratio = 0.40
        recommended_low = int((self.total_video_duration_seconds * low_density_ratio) / clip_duration)
        recommended_high = int((self.total_video_duration_seconds * high_density_ratio) / clip_duration)
        recommended_low = max(1, recommended_low)
        recommended_high = max(recommended_low, recommended_high)
        if num_clips < recommended_low:
            self.recommendation_label.setText(f"è®¾ç½®åˆç†ã€‚æ‚¨ä¹Ÿå¯ä»¥å°è¯•ç”Ÿæˆæ›´å¤šç‰‡æ®µä»¥æ•æ‰æ‰€æœ‰é«˜å…‰ã€‚")
        elif num_clips > recommended_high:
            self.recommendation_label.setText(f"æ³¨æ„: æ•°é‡å¯èƒ½åé«˜ï¼Œä¼šåŒ…å«éé«˜å…‰å†…å®¹ã€‚\nåŸºäº{total_minutes:.1f}åˆ†é’Ÿè§†é¢‘ï¼Œå»ºè®®ç”Ÿæˆ {recommended_low}-{recommended_high} ä¸ªé«˜è´¨é‡ç‰‡æ®µã€‚")
        else:
            self.recommendation_label.setText(f"è®¾ç½®åˆç†ã€‚å°†åœ¨çº¦{total_minutes:.1f}åˆ†é’Ÿçš„è§†é¢‘ä¸­å¯»æ‰¾é«˜å…‰ã€‚")

    def _set_controls_enabled(self, enabled: bool):
        if enabled:
            pass
        else:
            self.landscape_to_portrait_checkbox.setEnabled(False)
        for widget in [self.browse_button, self.api_key_edit, self.num_clips_spinbox,
                       self.clip_duration_spinbox, self.max_tasks_spinbox, self.start_button,
                       self.open_dir_button, self.language_combo, self.power_words_edit,
                       self.burn_subtitle_checkbox, self.precise_subtitle_checkbox,
                       self.add_bgm_checkbox, self.gpu_accel_checkbox,
                       self.whisper_model_combo, self.subtitle_lines_spinbox, self.max_chars_per_line_spinbox,
                       self.vcodec_combo, self.preset_combo, self.crf_spinbox, self.help_button]:
            if widget is not self.landscape_to_portrait_checkbox:
                widget.setEnabled(enabled)
        self.gpu_accel_checkbox.setEnabled(enabled and self.has_gpu)
        if enabled:
            self.update_vcodec_options()
            self.update_preset_options(self.vcodec_combo.currentText())
        self.stop_button.setEnabled(not enabled)
        self.start_button.setText("ğŸš€ å¼€å§‹ç”Ÿæˆ" if enabled else "æ­£åœ¨å¤„ç†ä¸­...")

    def start_processing(self):
        if not self.input_path_edit.text():
            QMessageBox.warning(self, "è­¦å‘Š", "è¯·å…ˆé€‰æ‹©ä¸€ä¸ªè§†é¢‘æ–‡ä»¶æˆ–ç›®å½•ï¼")
            return
        self.save_config()
        self._set_controls_enabled(False)
        self.log_output.clear()
        self.progress_bar.setValue(0)
        num_clips = self.num_clips_spinbox.value()
        tolerance = min(0.5, 0.15 + num_clips / 100)
        logger.info(f"æ ¹æ®ç”Ÿæˆæ•°é‡ {num_clips}ï¼Œè‡ªé€‚åº”æ—¶é•¿å®¹å¿åº¦è®¾ç½®ä¸º: {tolerance:.2f}")
        power_words_text = self.power_words_edit.text().strip()
        power_words = [word.strip() for word in re.split(r'[\s,ï¼Œ]+', power_words_text) if word.strip()]
        selected_lang_text = self.language_combo.currentText()
        language_code = self.supported_languages.get(selected_lang_text)
        selected_model_display = self.whisper_model_combo.currentText()
        whisper_model_name = self.whisper_models.get(selected_model_display, 'medium')
        params = {
            'input_path': self.input_path_edit.text(),
            'num_clips': num_clips,
            'clip_duration': self.clip_duration_spinbox.value(),
            'duration_tolerance': tolerance,
            'output_dir': self.output_dir_name,
            'config_path': self.config_path,
            'language': language_code,
            'power_words': power_words,
            'burn_subtitles': self.burn_subtitle_checkbox.isChecked(),
            'precise_subtitles': self.precise_subtitle_checkbox.isChecked(),
            'add_bgm': self.add_bgm_checkbox.isChecked(),
            'max_subtitle_lines': self.subtitle_lines_spinbox.value(),
            'max_chars_per_line': self.max_chars_per_line_spinbox.value(),
            'whisper_model': whisper_model_name,
            'vcodec': self.vcodec_combo.currentText(),
            'preset': self.preset_combo.currentText(),
            'crf': self.crf_spinbox.value(),
            'use_gpu': self.gpu_accel_checkbox.isChecked(),
            'convert_landscape_to_portrait': self.landscape_to_portrait_checkbox.isChecked()
        }
        self.thread = QThread()
        self.worker = Worker(params)
        self.worker.moveToThread(self.thread)
        self.worker.log_received.connect(self.append_log)
        self.thread.started.connect(self.worker.run)
        self.worker.finished.connect(self.on_finished)
        self.worker.error.connect(self.on_error)
        self.worker.progress.connect(self.set_progress)
        self.worker.finished.connect(self.thread.quit)
        self.worker.finished.connect(self.worker.deleteLater)
        self.thread.finished.connect(self.thread.deleteLater)
        self.thread.start()

    def stop_processing(self):
        if self.worker:
            reply = QMessageBox.question(self, 'ç¡®è®¤åœæ­¢', "ä»»åŠ¡æ­£åœ¨è¿›è¡Œä¸­ï¼Œç¡®å®šè¦å¼ºåˆ¶åœæ­¢å—ï¼Ÿ", QMessageBox.Yes | QMessageBox.No, QMessageBox.No)
            if reply == QMessageBox.Yes:
                logger.warning("ç”¨æˆ·è¯·æ±‚åœæ­¢ä»»åŠ¡ï¼Œæ­£åœ¨å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹...")
                self.worker.stop()
                self.thread.quit()
                self.thread.wait()
                logger.warning("ä»»åŠ¡å·²å¼ºåˆ¶åœæ­¢ã€‚")
                self.progress_bar.setValue(0)
                self._set_controls_enabled(True)
                if self.input_path_edit.text():
                    self.check_video_orientation(self.input_path_edit.text())
                self.append_log("------------------ ä»»åŠ¡å·²è¢«ç”¨æˆ·æ‰‹åŠ¨åœæ­¢ ------------------")
                self.thread, self.worker = None, None

    def open_output_directory(self):
        output_path = Path(self.output_dir_name)
        if not output_path.exists():
            output_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"è¾“å‡ºç›®å½• '{self.output_dir_name}' ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºã€‚")
        QDesktopServices.openUrl(QUrl.fromLocalFile(str(output_path.resolve())))
        logger.info(f"æ­£åœ¨æ‰“å¼€ç›®å½•: {output_path.resolve()}")

    def append_log(self, text):
        self.log_output.append(text)
        self.log_output.verticalScrollBar().setValue(self.log_output.verticalScrollBar().maximum())

    def set_progress(self, value):
        self.progress_bar.setValue(value)

    def on_finished(self):
        self.progress_bar.setValue(100)
        self._set_controls_enabled(True)
        if self.input_path_edit.text():
            self.check_video_orientation(self.input_path_edit.text())
        QMessageBox.information(self, "å®Œæˆ", f"æ‰€æœ‰ä»»åŠ¡å·²å¤„ç†å®Œæ¯•ï¼\nç‰‡æ®µå·²ä¿å­˜åˆ° {self.output_dir_name} æ–‡ä»¶å¤¹ã€‚")
        self.thread, self.worker = None, None

    def on_error(self, error_message):
        self._set_controls_enabled(True)
        if self.input_path_edit.text():
            self.check_video_orientation(self.input_path_edit.text())
        self.progress_bar.setValue(0)
        QMessageBox.critical(self, "é”™è¯¯", f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:\n{error_message}")
        self.thread, self.worker = None, None

    def closeEvent(self, event):
        self.save_config()
        if self.has_gpu:
            try:
                pynvml.nvmlShutdown()
                logger.info("pynvml å·²æˆåŠŸå…³é—­ã€‚")
            except pynvml.NVMLError:
                pass
        if self.worker:
            reply = QMessageBox.question(self, 'è­¦å‘Š', "ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­ï¼Œç¡®å®šè¦é€€å‡ºå—ï¼Ÿ", QMessageBox.Yes | QMessageBox.No, QMessageBox.No)
            if reply == QMessageBox.Yes:
                logger.warning("ç¨‹åºå³å°†é€€å‡ºï¼Œæ­£åœ¨ç»ˆæ­¢åå°ä»»åŠ¡...")
                self.worker.stop()
                self.thread.quit()
                self.thread.wait()
                event.accept()
            else:
                event.ignore()
        else:
            event.accept()

if __name__ == '__main__':
    multiprocessing.freeze_support()
    if getattr(sys, 'frozen', False) and sys.platform == "win32":
        multiprocessing.set_start_method('spawn')
        os.environ["PATH"] += os.pathsep + str(Path(sys.executable).parent)
    app = QApplication(sys.argv)
    app.setStyleSheet(STYLESHEET)
    main_win = MainWindow()
    main_win.show()
    sys.exit(app.exec_())
